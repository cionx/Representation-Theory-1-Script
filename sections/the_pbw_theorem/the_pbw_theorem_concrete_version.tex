\section{The Poincaré--Birkhoff--Witt Theorem, Concrete Version}
\label{section for concrete pbw}


\begin{convention}
	For this \lcnamecref{section for concrete pbw} we fix a Lie algebra~$\glie$ with basis~$(x_i)_{i \in I}$.
	For better readability we will denote for every element~$x$ of~$\glie$ the associated element of~$\Univ(\glie)$ by~$x$ instead of~$\class{x}$.
\end{convention}


\begin{remark}
	As a consequence of the upcoming theorem of Poincaré--Birkhoff--Witt we will see in \cref{embedding into uea} that the canonical homomorphism of Lie~algebras from~$\glie$ to~$\Univ(\glie)$ is actually injective.
	This will allow us to identify the Lie~algebra~$\glie$ with its image in~$\Univ(\glie)$.
	This will a posteriori justify our abuse of notation.
\end{remark}


\begin{fluff}
	Our results about the Weyl~algebra~$A$ and its associated graded algebra~$\gr(A)$ from~\cref{results about the weyl algerba} are based on one central observation:
	the generators~$x$ and~$y$ of~$A$ are of degree~$1$ while their commutator~$xy - yx = -1$ is of degree~$0$.
	The generator~$x$ and~$y$ therefore \enquote{commute up to smaller degree}, whence the associated element in~$\gr(A)$ actually commute.
	
	For the universal enveloping algebra~$\Univ(\glie)$ a similar situation occurs.
	For any two elements~$x$ and~$y$ of~$\glie$ the relation~$xy = yx + [x,y]$ holds in~$\Univ(\glie)$.
	The elements~$x$ and~$y$ hence ought to commute up to smaller degree with respect to the filtration on~$\Univ(\glie)$ from~\cref{examples for filtered algebras}.

	We should therefore be able to generalize our results from \cref{results about the weyl algerba} from the Weyl~algebra~$A$ to the universal enveloping algebra~$\Univ(\glie)$.
	This will lead us to the upcoming theorem of Poincaré--Birkhoff--Witt.

	We will start by making a basic observation, which then allows us to introduce some useful notation.
\end{fluff}


%\begin{fluff}
%  We already know that the universal enveloping algebra~$\Univ(\glie)$ is generated by the image of~$\glie$ in~$\Univ(\glie)$.
%  The algebra~$\Univ(\glie)$ is therefore generated by the basis elements~$x_i$ of~$\glie$.
%  This means that the momomials~$x_{i_1} \dotsm x_{i_n}$ with~$n \geq 0$,~$i_1, \dotsc, i_n \in I$ are vector space generators of~$\Univ(\glie)$.
%  By the above discussion we should hopefully be able to rearrange the terms~$x_{i_j}$ in these monomials without destroying the property of being a vector space generating set, similar to how we have done in the proof of~\cref{subspace spanned by monomials}.
%  We will see in \cref{pbw concrete generating part} that this is indeed the case.
%\end{fluff}


\begin{recall}
	Let~$V$ be a vector space, let~$W$ be a linear subspace of~$V$ and let~$v_1$ and~$v_2$ be two elements of~$V$.
	The notation
	\[
		v_1 \equiv v_2
		\pmod{W}
		\glsadd{mod elements}
	\]
	means that~$v_1$ and~$v_2$ become equal in the quotient vector space~$V/W$.
	More explicitely, the difference~$v_1 - v_2$ is contained in~$W$.
	Less explicitely, the elements~$v_1$ and~$v_2$ are \enquote{equal up to an element of~$W$}\index{equal up to an element of a subspace}.
\end{recall}


\begin{lemma}
	\label{rearranging lemma}
	Let~$x_1, \dotsc, x_n$ be elements of~$\glie$ and let~$\sigma$ be a permutation in~$\symm_n$.
	Then
	\[
		x_{\sigma(1)} \dotsm x_{\sigma(n)}
		\equiv
		x_1 \dotsm x_n
		\pmod{ \Univ(\glie)_{(n-1)} } \,.
	\]
\end{lemma}


\begin{proof}
	We have an action of the symmetric group~$\symm_n$ on the tensor power~$\glie^{\tensor n}$ given by
	\[
		(x_1 \tensor \dotsb \tensor x_n) \act \sigma
		=
		x_{\sigma(1)} \tensor \dotsb \tensor x_{\sigma(n)}
	\]
	for all~$x_1, \dotsc, x_n \in \glie$ and~$\sigma \in \symm_n$.
	We need to show that the multiplication map
	\[
		m
		\colon
		\glie^{\tensor n}
		\to
		\Univ(\glie) \,,
		\quad
		x_1 \tensor \dotsb \tensor x_n
		\mapsto
		x_1 \dotsm x_n
	\]
	satisfies the condition
	\[
		m(t \act \sigma)
		\equiv
		m(t)
		\pmod{\Univ(\glie)_{(n-1)}}
	\]
	for all~$t \in \glie^{\tensor n}$ and~$\sigma \in \symm_n$.
	This holds true if~$\sigma = 1$.
	We see that is sufficies to show this condition for simple transpositions, since the symmetric group~$\symm_n$ is generated by these transpositions.

	Let us therefore assume that~$\sigma$ is a simple transposition, say~$\sigma = (i, i+1)$.
	It follows from the identity
	\[
		x_i x_{i+1}
		=
		x_{i+1} x_i + [x_i, x_{i+1}]
	\]
	in~$\Univ(\glie)$ that
	\begin{align*}
		x_1 \dotsm x_n
		=
		x_1 \dotsm x_i x_{i+1} \dotsm x_n
		=
		x_1 \dotsm x_{i+1} x_i\dotsm x_n
		+
		x_1 \dotsm [x_i, x_{i+1}] \dotsm x_n \,.
	\end{align*}
	The term~$x_1 \dotsm [x_i, x_{i+1}] \dotsm x_n$ is contained in~$\Univ(\glie)_{(n-1)}$, which proves the \lcnamecref{rearranging lemma}.
\end{proof}


\begin{fluff}
	According to~\cref{rearranging lemma} we can rearrange for all elements~$x_1, \dotsc, x_n$ of~$\glie$ the monomial~$x_1 \dotsm x_n$ in~$\Univ(\glie)$ up to an error term of smaller degree.
	For this reording to be useful we want some canonical way in which to order the~$x_i$.
\end{fluff}


\begin{convention}
	We suppose in the following that~$\leq$ is a linear order on the index set~$I$, so that~$(I, \leq)$ is a linearly ordered set.
	\begin{enumerate}
		\item
			For every natural number~$n$ we set
			\[
				I^n
				\defined
				\{
					(i_1, \dotsc, i_n)
				\suchthat
					\text{$i_1, \dotsc, i_n \in I$ with~$i_1 \leq \dotsb \leq i_n$}
				\} \,,
				\glsadd{ordered n tuples}
			\]
			as well as~$I^{(n)} \defined \bigcup_{m=0}^n I^m$\glsadd{ordered leq n tuples}.
			We set overall~$I^* \defined \bigcup_{n \geq 0} I^n$\glsadd{ordered tuples}.
		\item
			For every tupel~$\alpha$ in~$I^*$ given by~$\alpha = (i_1, \dotsc, i_n)$ we define the associated \defemph{ordered monomial}~$x_\alpha$\index{ordered monomial} in~$\Univ(\glie)$ as
			\[
				x_\alpha
				\defined
				x_{i_1} \dotsm x_{i_n}  \,.
				\glsadd{ordered monomial}
			\]
		\item
			For any two elements~$\alpha$ and~$\beta$ of~$I^*$ we denote by~$\alpha \cdot \beta$\glsadd{ordered concatination} the tupel in~$I^*$ that results from the concatenation of~$\alpha$ and~$\beta$ by reordering of the entries\index{ordered concatination}\index{concatination}.
			More explicitely, if~$\alpha = (i_1, \dotsc, i_n) \in I^n$ and~$\beta = (j_1, \dotsc, j_m) \in I^m$ then the concatenation of~$\alpha$ and~$\beta$ is given by~$(i_1, \dotsc, i_n, j_1, \dotsc, j_m)$.
			By reordering the entries of this tuple in increasing order we arrive at the tupel~$\alpha \cdot \beta$.
		\item
			For every element~$j$ of~$I$ and every tupel~$\alpha$ in~$I^*$ we define the product~$j \cdot \alpha$ by identifying~$I$ with~$I^1$.
			More explicitely, if~$\alpha = (i_1, \dotsc, i_n)$ with~$i_1 \leq \dotsb \leq i_k \leq j \leq i_{k+1} \leq \dotsb \leq i_n$ then~$i \cdot \alpha = (i_1, \dotsc, i_k, j, i_{k+1}, \dotsc, i_n)$.
		\item
			For any element~$i$ of~$I$ and every tuple~$\alpha$ in~$I^*$ given by~$\alpha = (i_1, \dotsc, i_n)$ we write~$i \leq \alpha$\glsadd{element leq tuple} if~$i \leq i_1$.
			It then holds that~$i \cdot \alpha = (i, i_1, \dotsc, i_n)$.
			We have in particular for~$\alpha = ()$ that~$i \leq \alpha$ for every~$i \in I$, and~$i \cdot \alpha = (i)$.
	\end{enumerate}
\end{convention}


\begin{theorem}[Poincaré--Birkhoff--Witt\footnote{Named after Henri Poincaré (1854--1912), Garrett Birkhoff (1911--1996) and Ernst Witt (1911--1991).}, concrete version]\index{Poincaré--Birkhoff-Witt!concrete version}\index{PBW|see {Poincaré--Birkhoff--Witt}}
	\label{pbw concrete}
	The elements~$x_\alpha$ with~$\alpha \in I^*$ form a~{\basis{$\kf$}} of~$\Univ(\glie)$.%
	\footnote{This statement entails that these elements pairwise different, i.e. that~$x_\alpha \neq x_\beta$ whenever~$\alpha \neq \beta$.}
\end{theorem}


\begin{remark}
	The above basis of~$\Univ(\glie)$ can more explicitely be written as
	\[
		x_{i_1} \dotsm x_{i_n}
		\qquad
		\text{with~$n \geq 0$ and~$i_1, \dotsc, i_n \in I$ with~$i_1 \leq \dotsb \leq i_n$}
	\]
	and equivalently as
	\[
		x_{i_1}^{n_1} \dotsm x_{i_r}^{n_r}
		\qquad
		\text{with~$r \geq 0$,~$n_1, \dotsc, n_r \geq 1$ and~$i_1, \dotsc, i_r \in I$ with~$i_1 < \dotsb < i_r$} \,.
	\]
\end{remark}


\begin{example}
	\leavevmode
	\begin{enumerate}
		\item
			Let~$\glie$ be a finite-dimensional Lie algebra with basis~$x_1, \dotsc, x_r$.
			The universal enveloping algebra~$\Univ(\glie)$ has the ordered monomials~$x_1^{n_1} \dotsm x_r^{n_r}$ with~$n_1, \dotsc, n_r \geq 0$ as a basis.
		\item
			A basis of~$\Univ(\sllie(2, \kf))$ is given by the ordered monomials~$e^n h^m f^k$ with~$n, m, k \geq 0$.
	\end{enumerate}
\end{example}


\begin{fluff}
	To prove the concrete version of the PBW~theorem we need to show that the given ordered monomials generate~$\Univ(\glie)$ as a vector space and that they are linearly independet.
	We will use \cref{rearranging lemma} to prove the first assertion.
	We will actually show a slighty stronger statement in the upcoming \cref{pbw concrete generating part filtered part}.
	This will turn out to be useful for the abstract version of the PBW-theorem later on.
\end{fluff}


\begin{lemma}
	\label{pbw concrete generating part filtered part}
	The ordered monomials~$x_\alpha$ with~$\alpha$ in~$I^{(p)}$ are a vector space generating set of~$\Univ(\glie)_{(p)}$ for every~$p \geq 0$.
\end{lemma}


\begin{proof}
	We show the \lcnamecref{pbw concrete generating part filtered part} by induction over~$p$.
	It holds for~$p = 0$ because
	\[
		\Univ(\glie)_{(0)}
		=
		\gen{ 1 }_{\kf}
		=
		\gen{ x_{()} }_{\kf}
		=
		\gen{ x_\alpha \suchthat \alpha \in I^{(0)} }_{\kf}
	\]
	where~$()$ denotes the empty tupel, which is the only element of~$I^{(0)} = I^{0}$.
	
	Suppose now that by induction hypothesis the vector space~$\Univ(\glie)_{(p)}$ is spanned by all ordered monomials~$x_\alpha$ with~$\alpha \in I^{(p)}$.
	The vector space~$\Univ(\glie)_{(p+1)}$ is by definition spanned by all the monomials~$x_{i_1} \dotsm x_{i_{p+1}}$ with~$i_1, \dotsc, i_{p+1} \in I$.
	It hence sufficies to show that every such monomials can be expressed via the proposed generators~$x_\alpha$ with~$\alpha \in I^{(p+1)}$.
	We know from \cref{rearranging lemma} that
	\[
		x_{i_1} \dotsm x_{i_{n+1}}
		=
		x_\alpha + r
	\]
	for some element~$\alpha$ of~$I^{(p+1)}$ and some element~$r$ of~$\Univ(\glie)_{(p)}$ by rearrangement of the factors~$x_{i_j}$.
	We can apply the induction hypothesis to the term~$r$.
	We can thus express~$r$ as a linear combination of all those~$x_\beta$ with~$\beta$ in~$I^{(p)}$.
	The claim now follows because~$I^{(p)}$ is contained in~$I^{(p+1)}$.
\end{proof}


\begin{corollary}
	\label{generating for pbw}
	The ordered monomials~$x_\alpha$ with~$\alpha$ in~$I^*$ generate~$\Univ(\glie)$ as a vector space.
	\qed
\end{corollary}


\begin{lemma}
	\label{linear independence for pbw}
	The ordered monomials~$X_\alpha$ with~$\alpha$ in~$I^*$ are linearly independent.
\end{lemma}


\begin{fluff}
	To prove \cref{linear independence for pbw} we will use \cref{representation theory trick to construct a basis}.
	In the notation of \cref{representation theory trick to construct a basis} we will define the required~\module{$\Univ(\glie)$} structure on~$M$ by considering an action of~$\glie$ on~$M$.
	We will construct this action by considering a suitable filtration~$M = \bigcup_{n=0}^\infty M_{(n)}$ and inductively define actions
	\[
		\glie \times M_{(n)}
		\to
		M_{(n+1)} \,.
	\]
\end{fluff}


\begin{proof}[Proof of \cref{linear independence for pbw}]
	Let~$M$ be a vector space with basis~$X_\alpha$ with~$\alpha \in I^*$.
	For every natural number~$n$ we denote by~$M_{(n)}$ the linear subspace of~$M$ spanned by all those basis elements~$X_\alpha$ with~$\alpha \in I^{(n)}$.

	We show in the following that there exists a unique sequence~$(h_n)_{n \geq 0}$ of bilinear maps
	\[
		h_n
		\colon
		\glie \times M_{(n)}
		\to
		M_{(n+1)},
		\quad
		(x, m)
		\mapsto
		x \act m
	\]
	such that the following properties are satisfied.
	\begin{enumerate*}
		\item
			\label{pbw restriction coincides}
			The map~$h_{n+1}$ restricts for every~$n \geq 0$ to a map from~$\glie \times M_{(n)}$ to~$M_{(n+1)}$, and this restriction is precisely with the previous map~$h_n$.%
			\footnote{
				This condition actually follows from the other conditions by the uniqueness of the sequence~$(h_n)_{n \geq 0}$.
				See \cite[\S 17.4]{humphreys} for more details on this.
			}
		\item
			\label{pbw representation of lie algebra}
			$x_i \act x_j \act X_\alpha - x_j \act x_i \act X_\alpha = [x_i, x_j] \act X_\alpha$ for all~$i,j \in I$ and~$\alpha \in I^*$.
		\item
			\label{pbw essential condition}
			$x_i \act X_\alpha = X_{i \cdot \alpha}$ for all~$i \in I$ and~$\alpha \in I^*$ with $i \leq \alpha$.
		\item
			\label{pbw technical detail for construction}
			$x_i \act X_\alpha \equiv X_{i \cdot \alpha} \pmod{M_{(n)}}$ for all~$n \geq 0$,~$i \in I$,~$\alpha \in I^{(n)}$.
	\end{enumerate*}

	Let us point out that the notation~$x \act m$ with~$x \in \glie$ and~$m \in M$ is unambiguous by condition~\ref{pbw restriction coincides}.
	We will in the following construct the maps~$h_n$ by induction over~$n$.
	
	To define the map~$h_0$ we do not have to consider condition~\ref{pbw restriction coincides}.
	The linear subspace~$M_{(0)}$ is {\onedimensional} and spanned by the single element $X_{()}$.
	We see from condition~\ref*{pbw essential condition} that we need
	\[
		x_i \act X_{()}
		=
		X_{i \cdot ()}
		=
		X_i
	\]
	for all~$i \in I$.
	We take this as the definition for~$h_0$.
	Conditions~\ref{pbw essential condition} thus hold by definition of~$h_0$,~and condition~\ref{pbw technical detail for construction} follows from this.
	Condition~\ref{pbw representation of lie algebra} does not affect the case~$n = 0$.

	Let now~$n$ be a natural number and suppose that the action maps~$h_0, \dotsc, h_n$ have already been constructed.
	To construct the map~$h_{n+1}$ we need by to define the action~$x_i \act X_\alpha$ only for the elements~$\alpha$ of~$I^{n+1}$ by Condition~\ref*{pbw restriction coincides}.
	
	If~$i \leq \alpha$, then we need the equality
	\begin{equation}
		\label{formula for smaller case}
		x_i \act X_\alpha
		=
		X_{i \cdot \alpha}
	\end{equation}
	to ensure that condition~\ref{pbw essential condition} holds for~$h_{n+1}$.
	If on the other hand~$i \nleq \alpha$, then we can decompose the tupel~$\alpha$ as~$\alpha = j \cdot \beta$ where~$j$ is the first entry of~$\alpha$ and~$\beta$ is the rest of~$\alpha$.
	Then~$j \leq \beta$ and~$i > j$.
	It follows from Condition~\ref{pbw representation of lie algebra} and Condition~\ref{pbw essential condition} that we need the equalities
	\[
		x_i \act X_\alpha
		=
		x_i \act X_{j \cdot \beta}
		=
		x_i \act x_j \act X_\beta
		=
		x_j \act x_i \act X_\beta + [x_i, x_j] \act X_\beta  \,.
	\]
	The second summand~$[x_i, x_j] \act X_\beta$ is already defined by induction, and the term~$x_i \act X_\beta$ is also already defined by induction.
	We observe that by induction hypothesis condition~\ref{pbw technical detail for construction} holds for~$h_n$, whence
	\[
		x_i \act X_\beta
		\equiv
		X_{i \cdot \beta}
		\mod
		M_{(n)} \,.
	\]
	This means that
	\[
		x_i \act X_\beta = X_{i \cdot \beta} + R
	\]
	for some unique error term~$R$ in~$M_{(n)}$.
	It follows from the inequalities~$j < i$ and~$j \leq \beta$ that~$j \leq i \cdot \beta$.
	The term~$x_j \act X_{i \cdot \beta}$ needs therefore be given by
	\[
		x_j \act X_{i \cdot \beta}
		=
		X_{j \cdot i \cdot \beta}
		=
		X_{i \cdot j \cdot \beta}
		=
		X_{i \cdot \alpha}  \,.
	\]
	We find altogether that the term~$x_i \act X_\alpha$ needs to be given by
	\begin{align}
		x_i \act X_\alpha
		\notag
		&=
		x_i \act X_{j \cdot \beta}
		\notag
		\\
		&=
		x_i \act x_j \act X_\beta
		\notag
		\\
		&=
		x_j \act x_i \act X_\beta + [x_i, x_j] \act X_\beta
		\notag
		\\
		&=
		x_j \act (X_{i \cdot \beta} + R) + [x_i, x_j] \act X_\beta
		\notag
		\\
		&=
		x_j \act X_{i \cdot \beta} + x_j \act R + [x_i, x_j] \act X_\beta
		\notag
		\\
		&=
		X_{i \cdot \alpha} + x_j \act R + [x_i, x_j] \act X_\beta
		\label{formula for bigger case}
	\end{align}
	where~$j$ the first entry of~$\alpha$,~$\beta$ the rest of~$\alpha$ and the term~$R$ is given by~$R = x_i \act X_\beta - X_{i \cdot \beta}$.
	We take~\eqref{formula for bigger case} as the definition of~$x_i \act X_\alpha$ in the case~$i \nleq \alpha$.
	We altogether define~$x_i \act X_\alpha$ by~\eqref{formula for smaller case} or by~\eqref{formula for bigger case}, so that
	\begin{equation}
		\label{formula for both cases}
		x_i \act X_\alpha
		\defined
		\begin{cases*}
			X_{i \cdot \alpha}
			&
			if~$i \leq \alpha$,
			\\
			X_{i \cdot \alpha} + x_j \act R + [x_i, x_j] \act X_\beta
			&
			if~$i \nleq \alpha$ with~$\alpha = (j, \beta)$,~$R = x_i \act X_\beta - X_{i \act \beta} \in M_{(n)}$.
		\end{cases*}
	\end{equation}
	for all~$i \in I$ and~$\alpha \in I^{n+1}$.
	
	We overall defined~$h_{n+1}$ as the unique bilinear extension of~$h_n$ such that~$x_i \act X_\alpha$ is defined for the additional elements~$X_\alpha$ with~$\alpha \in I^{n+1}$ by~\eqref{formula for both cases}.
	We have also seen that the choices are forced upon us by the desired conditions.
	We have therefore shown the uniqueness of the map~$h_{n+1}$.

	We have constructed~$h_{n+1}$ as as extension of~$h_n$, whence condition~\ref{pbw restriction coincides} holds for~$h_{n+1}$.
	By using that~$h_n$ maps~$M_{(n)}$ into~$M_{(n+1)}$ and the formula~\eqref{formula for both cases} we also find that~$h_{n+1}$ maps~$M_{(n+1)}$ into~$M_{(n+2)}$.

	Condition~\ref{pbw essential condition} holds for~$h_{n+1}$ by the following case distinction:
	It holds for~$\alpha$ in~$I^{(n)}$ by the corresponding property of~$h_n$, and for~$\alpha$ in~$I^{(n+1)}$ because for~$i \leq \alpha$ the element~$x_i \act X_\alpha$ is defined via the formula~\eqref{formula for smaller case}.

	Condition~\ref{pbw technical detail for construction} holds for~$h_{n+1}$ by the following case distinction
	For~$\alpha$ in~$I^{(n)}$ it holds by induction hypothesis.
	For~$\alpha$ in~$I^{n+1}$ and~$i \leq \alpha$ it holds by condition~\ref{pbw essential condition}.
	For~$\alpha$ in~$I^{n+1}$ and~$i \nleq \alpha$ we have
	\begin{equation}
		\label{checking condition 3 for induction step}
		x_i \act X_\alpha - X_{i.\alpha}
		=
		x_j \act R + [x_i, x_j] \act X_\beta
	\end{equation}
	where~$j$ is the first entry of~$\alpha$, the tuple~$\beta$ in~$I^n$ is the rest of~$\alpha$, and~$R$ is the element of~$M_{(n+1)}$ given by~$R = x_i \act X_\beta - X_{i \cdot \beta}$.
	We need to show that the right hand side of the equation~\eqref{checking condition 3 for induction step} is contained in~$M_{(n+1)}$.
	The term~$[x_i, x_j] \act X_\beta$ is contained in~$M_{(n+1)}$ because~$X_\beta$ is contained in~$M_{(n)}$.
	The term~$R = x_i \act X_\beta - X_{i \cdot \beta}$ is contained in~$M_{(n)}$ because $X_\beta \equiv X_{i \cdot \beta} \pmod{M_{(n)}}$ by induction hypotheses.
	The term~$x_j \act R$ is therefore also contained in~$M_{(n+1)}$.
	Together, this shows that the right hand side of~\eqref{checking condition 3 for induction step} is indeed contained in~$M_{(n+1)}$.

	It remains to check condition~\ref{pbw representation of lie algebra} for~$h_{n+1}$.
	In other words, we need to check that
	\begin{equation}
		\label{ugly thing to show}
		x_i \act x_j \act X_\alpha - x_j \act x_i \act X_\alpha
		=
		[x_i, x_j] \act X_\alpha
	\end{equation}
	for all~$i, j \in I$ and~$\alpha \in I^{(n)}$.
	We may assume that~$\alpha$ is contained in~$I^n$ since the formula~\eqref{ugly thing to show} is otherwise covered by the induction hypothesis.
	The formula~\eqref{ugly thing to show} also holds for~$i = j$ becaues then both sides vanish.
	We may therefore assume in the following that~$i \neq j$.
	We show~\eqref{ugly thing to show} by an elaborate case distinction.
	The reader who is faint of heart may want to skip the rest of this proof and skip forward to happier topics.
	
	\begin{casedistinction}
		\item
			\label{first element is smaller than monomial}
			Suppose that~$i \leq \alpha$.
			We consider two subcases.
			\begin{casedistinction}
				\item
					Suppose that both~$i \leq \alpha$ and~$i < j$.
					It follows from the assumption~$i \leq \alpha$ that~$x_i \cdot X_\alpha = X_{i \cdot \alpha}$.
					The first entry of~$i \cdot \alpha$ is~$i$, whence it follows from the assumption~$i < j$ that~$j \nleq i \cdot \alpha$.
					It follows that the element
					\[
						x_j \act x_i \act X_\alpha
						=
						x_j \act X_{i \cdot \alpha}
					\]
					is given by
					\begin{align*}
						x_j \act X_{i \cdot \alpha}
						&=
						X_{j \cdot i \cdot \alpha}
						+ x_i \act R
						+ [x_j, x_i] \act X_\alpha
						\\
						&=
						X_{j \cdot i \cdot \alpha}
						+ x_i \act ( x_j \act X_\alpha - X_{j \act \alpha} )
						+ [x_j, x_i] \act X_\alpha
						\\
						&=
						X_{j \cdot i \cdot \alpha}
						+ x_i \act x_j \act X_\alpha
						- x_i \act X_{j \act \alpha}
						+ [x_j, x_i] \act X_\alpha
						\\
						&=
						X_{j \cdot i \cdot \alpha}
						+ x_i \act x_j \act X_\alpha
						- x_i \act X_{j \act \alpha}
						- [x_i, x_j] \act X_\alpha
					\end{align*}
					where~$R = x_j \act X_\alpha - X_{j \act \alpha}$.
					By rearranging this equation we find that
					\begin{align*}
						{}&
						x_i \act x_j \act X_\alpha - x_j \act x_i \act X_\alpha
						\\
						={}&
						x_i \act x_j \act X_\alpha - x_j \act X_{i \cdot \alpha}
						\\
						={}&
						x_i \act x_j \act X_\alpha
						- X_{j \cdot i \cdot \alpha}
						- x_i \act x_j \act X_\alpha
						+ x_i \act X_{j \act \alpha}
						+ [x_i, x_j] \act X_\alpha
						\\
						={}&
						- X_{j \cdot i \cdot \alpha}
						+ x_i \act X_{j \act \alpha}
						+ [x_i, x_j] \act X_\alpha \,.
					\end{align*}
					It follows from the assumptions~$i < j$ and~$i \leq \alpha$ that~$i \leq j \cdot \alpha$.
					We therefore have
					\[
						x_i \cdot X_{j \cdot \alpha}
						=
						X_{i \cdot j \cdot \alpha}
						=
						X_{j \cdot i \cdot \alpha} \,.
					\]
					It therefore further follows that
					\[
						x_i \act x_j \act X_\alpha - x_j \act x_i \act X_\alpha
						=
						- X_{j \cdot i \cdot \alpha}
						+ x_i \act X_{j \act \alpha}
						+ [x_i, x_j] \act X_\alpha
						=
						[x_i, x_j] \act X_\alpha \,.
					\]
				\item
					Suppose that both~$i \leq \alpha$ and~$i > j$.
					It follows that~$j \leq \alpha$.
					We also have~$j < i$ by assumption.
					It follows from the previous case that
					\[
						x_j \act x_i \act X_\alpha - x_i \act x_j \act X_\alpha
						=
						[x_j, x_i] \act X_\alpha \,.
					\]
					It follows that
					\begin{align*}
						\SwapAboveDisplaySkip
						[x_i, x_j].X \act \alpha
						&=
						-[x_j, x_i] \act X_\alpha
						\\
						&=
						- (x_j \act x_i \act X_\alpha - x_i \act x_j \act X_\alpha)
						\\
						&=
						x_i \act x_j \act X_\alpha - x_j \act x_i \act X_\alpha \,.
					\end{align*}
			\end{casedistinction}
		\item
			\label{second element is smaller than monomial}
			Suppose that~$j \leq \alpha$.
			It follows from \cref{first element is smaller than monomial} that
			\[
				x_j \act x_i \act X_\alpha - x_i \act x_j \act X_\alpha
				=
				[x_j, x_i] \act X_\alpha \,,
			\]
			and thus
			\begin{align*}
				\SwapAboveDisplaySkip
				[x_i, x_j] \act X_\alpha
				&=
				-[x_j, x_i] \act X_\alpha
				\\
				&=
				- (x_j \act x_i \act X_\alpha - x_i \act x_j \act X_\alpha)
				\\
				&=
				x_i \act x_j \act X_\alpha - x_j \act x_i \act X_\alpha \,.
			\end{align*}
		\item
			Suppose that both~$i \nleq \alpha$ and~$j \nleq \alpha$
			This case cannot happen for~$n = 0$ because then~$\alpha = ()$ (since~$\alpha$ is an element of~$I^n = I^0$) and thus~$i \leq () = \alpha$.
			We may therefore assume in the following that~$n \geq 1$.

			We can thus split up the tuple~$\alpha$ as~$\alpha = k \cdot \beta$ where~$k$ is the first entry of~$\alpha$ and~$\beta$ is the rest of~$\alpha$.
			We then have
			\[
				X_\alpha
				=
				X_{k \cdot \beta}
				=
				x_k \act X_\beta \,.
			\]
			The tupel~$\beta$ is contained in~$I^{n-1}$.
			We therefore know from the induction hypothesis that
			\[
				x_j \act X_\alpha
				=
				x_j \act x_k \act X_\beta
				=
				x_k \act x_j \act X_\beta 
				+ [x_j, x_k] \act X_\beta  \,.
			\]
			By acting with~$x_i$ on this equation we find the equality
			\begin{equation}
				\label{long equation}
				x_i \act x_j \act X_\alpha
				=
				x_i \act x_k \act x_j \act X_\beta
				+ x_i \act [x_j,x_k] \act X_\beta \,.
			\end{equation}
			Let~$Y \defined x_j \act X_\beta$.

			\begin{claim}
				We have
				\begin{equation}
					\label{wanted equation for pbw}
					x_i \act x_k \act Y
					=
					x_k \act x_i \act Y
					+ [x_i, x_k] \act Y \,.
				\end{equation}
			\end{claim}

			\begin{proof}
				The tupel~$\beta$ is contained in~$M_{(n-1)}$, so it follows from condition~\ref{pbw technical detail for construction} that
				\[
					Y
					=
					x_j \act X_\beta
					\equiv
					X_{j \cdot \beta}
					\mod
					M_{(n-1)} \,.
				\]
				The difference~$R \coloneqq Y - X_{j \cdot \beta}$ is therefore contained in~$M_{(n-1)}$.
				We can now decompose the element~$Y$ as
				\[
					Y
					=
					X_{j \cdot \beta} + R  \,.
				\]
				The desired identity~\eqref{wanted equation for pbw} is linear in~$Y$.
				To show~\eqref{wanted equation for pbw} we will therefore show the two identities
				\begin{equation}
					\label{wanted equation for pbw small for monomial}
					x_i \act x_k \act X_{j \cdot \beta}
					=
					x_k \act x_i \act X_{j \cdot \beta}
					+ [x_i, x_k] \act X_{j \cdot \beta} \,.
				\end{equation}
				and
				\begin{equation}
					\label{wanted equation for pbw small for rest}
					x_i \act x_k \act R
					=
					x_k \act x_i \act R
					+ [x_i, x_k] \act R \,.
				\end{equation}
				Identity~\eqref{wanted equation for pbw small for rest} holds by induction hypothesis because~$R$ is contained in~$M_{(n-1)}$.
				For identity~\eqref{wanted equation for pbw small for monomial} we recall that~$k$ is the first term of~$\alpha$.
				Thus~$k \leq \beta$.
				It also follows from the assumption~$j \nleq \alpha$ that~$j > k$.
				It now follows from~$k < j$ and~$k \leq \alpha$ that~$k \leq j \cdot \beta$.
				Identity~\ref{wanted equation for pbw small for monomial} does therefore follows from Case~2.
				We have altogether proven identity~\eqref{wanted equation for pbw}.
			\end{proof}

			By inserting the identity~\eqref{wanted equation for pbw} into the equality~\eqref{long equation} we find that
			\begin{align*}
				x_i \act x_j \act X_\alpha
				&=
				x_i \act x_k \act x_j \act X_\beta
				+ x_i \act [x_j, x_k] \act X_\beta
				\\
				&=
				x_i \act x_k \act Y
				+ x_i \act [x_j, x_k] \act X_\beta
				\\
				&=
				x_k \act x_i \act Y
				+ [x_i, x_k] \act Y
				+ x_i \act [x_j, x_k] \act X_\beta
				\\
				&=
				x_k \act x_i \act x_j \act X_\beta
				+ [x_i, x_k] \act x_j \act X_\beta
				+ x_i \act [x_j, x_k] \act X_\beta
			\end{align*}
			By swapping the roles of~$i$ and~$j$ we also find that
			\[
				x_j \act x_i \act X_\alpha
				=
				x_k \act x_j \act x_i \act X_\beta
				+ [x_j, x_k] \act x_i \act X_\beta
				+ x_j \act [x_i, x_k] \act X_\beta \,.
			\]
			It follows that
			\begin{align*}
				\SwapAboveDisplaySkip
				{}&
				x_i \act x_j \act X_\alpha
				- x_j \act x_i \act X_\alpha
				\\
				={}&
				x_k \act x_i \act x_j \act X_\beta
				+ [x_i, x_k] \act x_j \act X_\beta
				+ x_i \act [x_j, x_k] \act X_\beta
				\\
				{}&
				- x_k \act x_j \act x_i \act X_\beta
				- [x_j, x_k] \act x_i \act X_\beta
				- x_j \act [x_i, x_k] \act X_\beta \,.
			\end{align*}
			We know from the induction hypothesis the equalities
			\begin{align*}
				x_i \act x_j \act X_\beta
				- x_j \act x_i \act X_\beta
				&=
				[x_i, x_j] \act X_\beta \,,
				\\
				[x_i, x_k] \act x_j \act X_\beta
				- x_j \act [x_i, x_k] \act X_\beta
				&=
				[ [x_i, x_k], x_j ] \act X_\beta \,,
				\\
				x_i \act [x_j, x_k] \act X_\beta
				- [x_j, x_k] \act x_i \act X_\beta
				&=
				[ x_i, [x_j, x_k] ] \act X_\beta \,,
			\end{align*}
			and thus
			\begin{align*}
				\SwapAboveDisplaySkip
				{}&
				x_k \act x_i \act x_j \act X_\beta
				+ [x_i, x_k] \act x_j \act X_\beta
				+ x_i \act [x_j, x_k] \act X_\beta
				\\
				{}&
				- x_k \act x_j \act x_i \act X_\beta
				- [x_j, x_k] \act x_i \act X_\beta
				- x_j \act [x_i, x_k] \act X_\beta \,.
				\\
				={}&
				x_k \act [x_i, x_j] \act X_\beta
				+ [[x_i, x_k], x_j] \act X_\beta
				+ [x_i, [x_j, x_k]] \act X_\beta \,.
				\\
				={}&
				x_k \act [x_i, x_j] \act X_\beta
				+ \bigl( [[x_i, x_k], x_j] + [x_i, [x_j, x_k]] \bigr) \act X_\beta
			\end{align*}
			It follows from the Jacobi~identity that
			\begin{align*}
				[[x_i, x_k], x_j] + [x_i, [x_j, x_k]]
				&=
				- [[x_k, x_i], x_j] - [x_i, [x_k, x_j]]
				\\
				&=
				- [[x_k, x_i], x_j] - [x_i, [x_k, x_j]]
				\\
				&=
				- [x_k, [x_i, x_j]] \,,
			\end{align*}
			and thus
			\begin{align*}
				\SwapAboveDisplaySkip
				{}&
				x_k \act [x_i, x_j] \act X_\beta
				+ \bigl( [[x_i, x_k], x_j] + [x_i, [x_j, x_k]] \bigr) \act X_\beta
				\\
				={}&
				x_k \act [x_i, x_j] \act X_\beta
				- [x_k, [x_i, x_j] ] \act X_\beta \,.
			\end{align*}
			By using the induction hypothesis and that~$\alpha = k \cdot \beta$ with~$k \leq \beta$ we find that
			\begin{align*}
				[x_k, [x_i, x_j]] \act X_\beta
				&=
				x_k \act [x_i, x_j] \act X_\beta
				- [x_i, x_j] \act x_k \act X_\beta
				\\
				&=
				x_k \act [x_i, x_j] \act X_\beta
				- [x_i, x_j] \act X_{k \cdot \beta}
				\\
				&=
				x_k \act [x_i, x_j] \act X_\beta
				- [x_i, x_j] \act X_{\alpha} \,.
			\end{align*}
			It thus follows that
			\begin{align*}
				{}&
				x_k \act [x_i, x_j] \act X_\beta
				- [x_k, [x_i, x_j] ] \act X_\beta
				\\
				={}&
				x_k \act [x_i, x_j] \act X_\beta
				- x_k \act [x_i, x_j] \act X_\beta
				+ [x_i, x_j] \act X_{\alpha}
				\\
				={}&
				[x_i, x_j] \act X_{\alpha} \,.
			\end{align*}
			By putting all of these steps together we find that
			\begin{align*}
				x_i \act x_j \act X_\beta
				- x_j \act x_i \act X_\beta
				&=
				[x_i, x_j] \act X_{\alpha} \,.
			\end{align*}
	\end{casedistinction}
	We have thus finally proven condition~\ref{pbw technical detail for construction} for the induction step.

	We have now constructed the maps~$h_n$ and shown their uniqueness. 
	Condition~\ref{pbw restriction coincides} ensures that thes maps combine into a single bilinear map
	\[
		h
		\colon
		\glie \times M
		\to
		M \,.
	\]
	Condition~\ref{pbw representation of lie algebra} means that~$h$ is an action of~$\glie$ on~$M$, making~$M$ into a representation of~$\glie$.
	This action corresponds to an~\module{$\Univ(\glie)$} structure on~$M$.
	For every tuple~$\alpha$ in~$I^*$ with~$\alpha = (i_1, \dotsc, i_n)$ we have~$i_1 \leq \dotsb \leq i_n$ and thus
	\begin{align*}
		\SwapAboveDisplaySkip
		x_\alpha . X_{()}
		&=
		x_{i_1} \dotsm x_{i_n} \cdot X_{()}
		\\
		&=
		x_{i_1} \dotsm x_{i_{n-1}} \cdot X_{(i_n)}
		\\
		&=
		x_{i_1} \dotsm x_{i_{n-2}} \cdot X_{(i_{n-1}, i_n)}
		\\
		&=
		\dotsb
		\\
		&=
		X_{(i_1, \dotsc, i_n)}
		\\
		&=
		X_\alpha \,.
	\end{align*}
	It now follows from the linear independence of the elements~$X_\alpha$ with~$\alpha$ in~$I^*$ that the elements~$x_\alpha$ with~$\alpha \in I^*$ are linearly independent.
\end{proof}


\begin{proof}[Proof of \cref{pbw concrete}]
	It follows from \cref{generating for pbw} that the given monomials are a vector space generating set, and they are linearly indpendent by \cref{linear independence for pbw}.
\end{proof}


\begin{corollary}
	\label{pbw concrete basis part filtered part}
	For every~$n \geq 0$ the monomials~$x_\alpha$ with~$\alpha$ in~$I^{(n)}$ form a basis of~$\Univ(\glie)_{(n)}$.
\end{corollary}


\begin{proof}
	This follows from \cref{pbw concrete generating part filtered part} and \cref{linear independence for pbw}.
\end{proof}


\begin{remark}[The diamond lemma]\index{diamond lemma}
	The above proof of the PBW~theorem relies on a tactic from representation theory:
	constructing an action on a suitable vector space for which the proposed basis elements act linearly independently (on some fixed element).

	We may ask if there is instead a proof of the PBW~theorem that continues to study the commutator relation~$x_j x_i = x_i x_j + [x_i, x_j]$ with~$i, j \in I$ by exploiting that the commutator~$[x_i, x_j]$ ought to be of smaller degree than~$x_j x_i$ and~$x_i x_j$.
	A similar question may also be asked for the Weyl~algebra\index{Weyl algebra}.
	
	One way to do this is via the \defemph{diamond lemma} that was introduced in~\cite{diamond_lemma}.
	Suppose that we are given a~\algebra{$\kf$}~$A$ via generators~$x_i$ with~$i \in I$ and set of relations~$S$, where~$S$ a subset of the free algebra~$\kf\gen{x_i \suchthat i \in I}$.
	In other words, we have
	\[
		A
		=
		\kf\gen{x_i \suchthat i \in I}
		/
		\ideal{ S } \,.
	\]
	Then the diamond lemma gives a sufficient criterion for showing that~$A$ admits a basis consisting of monomials in the~$x_i$, and also tells us how these monomials can be choosen.
	
	The main idea behind the diamond lemma is simple:
	one needs to be able to resolve ambiguities\index{ambiguity}.
	Let us be a bit more explicit:
	We can represent every element of~$A$ as a linear combination of monomials in the~$x_i$, and hence by an element~$z$ of~$\kf\gen{x_i \suchthat i \in I}$.
	Every relation~$\sigma$ between the generators, i.e. element of~$S$, can be seen as a rewriting rule.
	This gives us linear \enquote{rewriting maps}
	\[
		r_\sigma
		\colon
		\kf\gen{x_i \suchthat i \in I}
		\to
		\kf\gen{x_i \suchthat i \in I}
	\]
	for all~$\sigma \in S$.
	To apply the diamond lemma one need to ensure two conditions.
	\begin{itemize}
		\item
			Whenever we apply rewritings~$r_{\sigma_1}, r_{\sigma_2}, \dotsc$ to an element of~$\kf\gen{x_i \suchthat i \in I}$ we want this process to stabilize after finitely many step.
		\item
			We need to be resolve ambiguities:
			Given an element~$z$ of~$\kf\gen{x_i \suchthat i \in I}$ we can often apply different rewriting rules~$r_\sigma$ and~$r_\tau$ to it, resulting in different results~$r_\sigma(z)$ and~$r_\tau(z)$.
			We want to reconcile these different results by reducing both~$r_\sigma(z)$ and~$r_\tau(z)$ under finitely many rewritings to the same result~$z'$.
			We may visualize this process by the following diagram:
			\[
				\begin{tikzcd}[column sep = small]
					{}
					&
					z
					\arrow{dl}
					\arrow{dr}
					&
					{}
					\\
					r_\sigma(z)
					\arrow{dr}
					&
					{}
					&
					r_\tau(z)
					\arrow{dl}
					\\
					{}
					&
					z'
					&
					{}
				\end{tikzcd}
			\]
			The shape of this diagram motivates the name \enquote{diamond lemma}.
	\end{itemize}
	
	The diamond is neither hard to state nor to prove, despite its usefulness.
	We strongly encourage the reader to check out the original paper \cite{diamond_lemma}.
	The main theorem and its proof in \cite[\S 1]{diamond_lemma} takes only three pages and is self-contained.
	In \cite[\S 3]{diamond_lemma} the PBW~theorem is then proven as an example.
\end{remark}





