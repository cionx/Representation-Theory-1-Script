\section{The Killing form and Cartanâ€™s criterion}





\subsection{Associative bilinear forms and the Killing form}


\begin{defi}
 Let $\g$ be a Lie algebra over an arbitrary field $k$. A bilinear form $\beta \colon \g \times \g \to k$ is called \emph{associative} if
 \[
  \beta(x,[y,z]) = \beta([x,y],z) \quad \text{for all $x,y,z \in \g$}.
 \]
\end{defi}


\begin{rem}
 As $\g$ acts on itself by the adjoint representation it also acts on $(\g \otimes_k \g)^*$ such that for every $\beta \in (\g \otimes_k \g)^*$ and all $x,y,z \in \g$
 \begin{align*}
  (y.\beta)(x \otimes z)
  &= -\beta(y.(x \otimes z))
  = -\beta((y.x) \otimes z + x \otimes (y.z)) \\
  &= -\beta([y,x] \otimes z) -\beta(x \otimes [y,z])
  = \beta([x,y] \otimes z) - \beta(x \otimes [y,z]).
 \end{align*}
 
 \begin{samepage} Identifying $(\g \otimes_k \g)^*$ with the bilinear forms on $\g$ via the universal property of the tensor product it follows that a bilinear form $\beta \colon \g \times \g \to k$ is associative if and only if
 \[
  y.\beta = 0 \quad \text{for every $y \in \g$}.
 \]
 Because of this associative bilinear forms on $\g$ are also called \emph{invariant}. \end{samepage}
\end{rem}



\begin{defi}
 Let $V$ be a vector space and $\beta \colon \g \times \g \to k$ a symmetric bilinear form. Then
 \[
  \rad \beta \coloneqq \{x \in V \mid \text{$\beta(x,y) = 0$ for every $y \in V$}\}
 \]
 is the \emph{radical} of $\beta$.
\end{defi}


\begin{lem}\label{lem: orthogonal complement of an ideal is again an ideal}
 Let $\g$ be a Lie algebra over an arbitrary field $k$ and \mbox{$\beta \colon \g \times \g \to k$} a symmetric and associative bilinear form. For any ideal $I \subideal \g$ the orthogonal complement
 \[
  I^\perp \coloneqq \{y \in \g \mid \text{$\beta(x,y) = 0$ for every $x \in I$}\}
 \]
 is also an ideal in $\g$. In particular $\rad \beta = \g^\perp$ is an Ideal in $\g$.
\end{lem}
\begin{proof}
 For $z \in g$ and $y \in I^\perp$ it follows for every $x \in I$ that $[x,y] = 0$ and thus
 \[
  \beta(x,[y,z]) = \beta([x,y],z) = 0.
  \qedhere
 \]
\end{proof}


\begin{rem}
 The proof of Lemma~\ref{lem: orthogonal complement of an ideal is again an ideal} did not use that $\beta$ is symmetric. This artficial restraint is only there to simplify the situation and notation (we do not need to distinguish between orthogonal complements from the left and from the right.) The main example of an associative bilinear form will be the Killing form, which is symmetric, so this assumption will pose no problems to us.
\end{rem}


\begin{defi}
 Let $\g$ be a finite dimensional Lie algebra over an arbitrary field $k$. The \emph{Killing form} of $\g$ is the bilinear form
 \[
  \kappa \colon \g \times \g \to k
  \quad\text{with}\quad
  \kappa(x,y) = \tr(\ad(x) \ad(y))
  \quad\text{for all $x,y \in \g$}.
 \]
\end{defi}


\begin{lem}\label{lem: killing form is associative and symmetric}
 The Killing form $\kappa$ of an finite dimensional Lie algebra $\g$ over an arbitrary field $k$ is associative and symmetric.
\end{lem}
\begin{proof}
 Recall from linear algebra that for any finite dimensional $k$-vector space $V$ and all endomorphisms $f_1, \dotsc, f_n \in \End_k(V)$
 \[
  \tr(f_1 \dotsm f_n) = \tr(f_2 \dotsm f_n f_1).
 \]
 For all $x,y \in \g$ it follows that
 \[
  \kappa(x,y) = \tr(\ad(x) \ad(y)) = \tr(\ad(y) \ad(x)) = \kappa(y,x),
 \]
 so $\kappa$ is symmetric. For all $x,y,z$ it follows that
 \begin{align*}
  \kappa(x,[y,z])
  &= \tr(\ad(x) \ad([y,z]))
  = \tr(\ad(x) [\ad(y), \ad(z)]) \\
  &= \tr(\ad(x) (\ad(y) \ad(z) - \ad(z) \ad(y))) \\
  &= \tr(\ad(x) \ad(y) \ad(z)) - \tr(\ad(x) \ad(z) \ad(y)) \\
  &= \tr(\ad(x) \ad(y) \ad(z)) - \tr(\ad(y) \ad(x) \ad(z)) \\
  &= \tr((\ad(x) \ad(y)- \ad(y) \ad(x)) \ad(z)) \\
  &= \tr([\ad(x), \ad(y)] \ad(z))
  = \tr(\ad([x,y]) \ad(z))
  = \kappa([x,y],z).
 \qedhere
 \end{align*}
\end{proof}


\begin{expl}\label{expl: Killing form of gl_n}
 Let $\g \coloneqq \gl_n(k)$ for some arbitrary field $k$. Then
 \[
  \kappa(x,y) = 2n\tr(xy) - 2(\tr x)(\tr y) \eqqcolon \beta(x,y).
  \quad \text{for all $x,y \in \g$}.
 \]
 To see this let $(e_{ij})_{i,j=1,\dotsc,n}$ be the standard basis of $\gl_n(k)$, i.e.\ the $(i,j)$-th entry of $e_{ij}$ is $1$, all other entries are $0$ (hence $e_{ij} e_k = \delta_{jk} e_i$ for every $j = 1, \dotsc, n$ where $(e_1, \dotsc, e_n)$ is the standard basis of $k^n$). In particular
 \begin{equation}\label{eqn: product of elementary matrices}
  e_{ij} e_{kl} = \delta_{jk} e_{il} \quad \text{for all $i,j,k,l = 1, \dotsc, n$}.
 \end{equation}

 To show that $\kappa = \beta$ is sufficies to show that
 \[
  \kappa(e_{ij}, e_{kl}) = \beta(e_{ij}, e_{kl})
  \quad \text{for all $i,j,k,l = 1, \dotsc, n$},
 \]
 as both $\kappa$ and $\beta$ are bilinear forms on $\g$. For all $k,l,g,h = 1, \dotsc, n$ it follows from \eqref{eqn: product of elementary matrices} that
 \[
  \ad(e_{kl})(e_{gh})
  = [e_{kl}, e_{gh}]
  = e_{kl} e_{gh} - e_{gh} e_{kl}
  = \delta_{lg} e_{kh} - \delta_{kh} e_{gl}.
 \]
 It follows that for all $i,j,k,l,g,h = 1, \dotsc, n$
 \begin{align*}
  \ad(e_{ij})\ad(e_{kl})(e_{gh})
  &= \ad(e_{ij})(\delta_{lg} e_{kh} - \delta_{kh} e_{gl})
  = \delta_{lg} \ad(e_{ij})(e_{kh}) - \delta_{kh} \ad(e_{ij})(e_{gl}) \\
  &= \delta_{lg}(\delta_{jk} e_{ih} - \delta_{ih} e_{kj}) - \delta_{kh}(\delta_{jg} e_{il} - \delta_{il} e_{gj}) \\
  &= \delta_{jk}\delta_{lg} e_{ih} - \delta_{ih} \delta_{lg} e_{kj} - \delta_{jg} \delta_{kh} e_{il} + \delta_{il} \delta_{kh} e_{gj}
 \end{align*}
 and the coefficient of $e_{gh}$ in this expression is
 \[
  a_{gh}
  = \delta_{jk} \delta_{lg} \delta_{ig}
    + \delta_{il} \delta_{kh} \delta_{jh}
    - \delta_{ih} \delta_{lg} \delta_{kg} \delta_{jh}
    - \delta_{jg} \delta_{kh} \delta_{ig} \delta_{hl}.
 \]
 It follows that for all $i,j,k,l = 1, \dotsc, n$
 \begin{align*}
  \kappa(e_{ij}, e_{kl})
  &= \sum_{g,h=1}^n ( \delta_{jk} \delta_{lg} \delta_{ig}
     + \delta_{il} \delta_{kh} \delta_{jh}
     - \delta_{ih} \delta_{lg} \delta_{kg} \delta_{jh}
     - \delta_{jg} \delta_{kh}\delta_{ig} \delta_{hl} ) \\
  &= \sum_{g,h=1}^n \delta_{jk} \delta_{lg} \delta_{ig}
     + \sum_{g,h=1}^n \delta_{il} \delta_{kh} \delta_{jh}
     - \sum_{g,h=1}^n \delta_{ih} \delta_{lg} \delta_{kg} \delta_{jh}
     - \sum_{g,h=1}^n\delta_{jg} \delta_{kh}\delta_{ig} \delta_{hl} \\
  &= n \delta_{jk} \delta_{il} + n \delta_{il} \delta_{jk} - \delta_{ij} \delta_{kl} - \delta_{ij} \delta_{kl}
  = 2n \delta_{il} \delta_{jk} - 2 \delta_{ij} \delta_{kl} \\
  &= 2n \delta_{jk} (\tr e_{il})  - 2 (\tr e_{ij})(\tr e_{kl})
  = 2n \tr(e_{ij} e_{kl}) - 2(\tr e_{ij})(\tr e_{kl}) \\
  &= \beta(e_{ij}, e_{kl}).
 \end{align*}
\end{expl}


\begin{rem}
 If $\g$ is a Lie algebra and $\rho \colon \g \to \gl(V)$ a finite dimensional representation of $\g$ then the corresponding trace form $\phi_\rho$ is defined as
 \[
  \phi_\rho(x,y) \coloneqq \tr(\rho(x) \rho(y)) \quad \text{for all $x,y \in \g$}.
 \]
 Replacing $\ad$ with $\rho$ in the proof of Lemma~\ref{lem: killing form is associative and symmetric} shows that $\phi_\rho$ is an associative and symmetric bilinear form on $\g$. The Killing form $\kappa$ of $\g$ is then just the special case $\kappa = \phi_{\ad}$.
\end{rem}


\begin{lem}\label{lem: restriction of the Killing form to an ideal}
 Let $\g$ be a finite-dimensional Lie algebra over an arbitrary field $k$. Then for any ideal $I \subseteq \g$ the Killing form $\kappa_I$ is given by restriction of the Killing form $\kappa_\g$ to $I$, i.e.\ $\kappa_I = \kappa_\g|_{I \times I}$.
\end{lem}
\begin{proof}
 Let $x, y \in I$. Then $I$ is $\ad_\g(x)$-invariant. Let $(x_1, \dotsc, x_r)$ be a basis of $I$ and $(x_1, \dotsc, x_r, x_{r+1}, \dotsc, x_s)$ one of $\g$. With respect to the basis $(x_1, \dotsc, x_r)$ of $I$ the endomorphism $\ad_I(x)$ is represented by a matrix $A_x \in \Mat_r(k)$ and $\ad_I(y)$ is represented by a matrix $A_y \in \Mat_r(k)$. As $I \subideal \g$ is in ideal it follows that $\im \ad_\g(x) \subseteq \g$ and $\im \ad_\g(y) \subseteq \g$, which is why with respect to the basis $(x_1, \dotsc, x_s)$ the endomorphism $\ad_\g(x)$ and $\ad_\g(y)$ are represented by a matrices
 \[
  C_x = \begin{pmatrix} A_x & B_x \\ 0 & 0 \end{pmatrix} \in \Mat_s(k)
  \quad\text{and}\quad
  C_y = \begin{pmatrix} A_y & B_y \\ 0 & 0 \end{pmatrix} \in \Mat_s(k)
 \]
 for some matrices $B_x, B_y \in \Mat_{s-r,r}(k)$. Hence
 \begin{align*}
  \kappa_\g(x,y)
  &= \tr(\ad_\g(x) \ad_\g(y))
  = \tr\left( \begin{pmatrix} A_x & B_x \\ 0 & 0 \end{pmatrix} \begin{pmatrix} A_y & B_y \\ 0 & 0 \end{pmatrix} \right) \\
  &= \tr \begin{pmatrix} A_x A_y & A_x B_y \\ 0 & 0 \end{pmatrix}
  = \tr(A_x A_y)
  = \tr(\ad_I(x) \ad_I(y))
  = \kappa_I(x,y).
 \qedhere
 \end{align*}
\end{proof}


\begin{expl}
 Let $\g \coloneqq \sll_n(k) = [\gl_n(k), \gl_n(k)]$. As seen in example~\ref{expl: Killing form of gl_n} the Killing form of $\gl_n(k)$ is given by
 \[
  \kappa_{\gl_n(k)}(x,y) = 2n \tr(xy) - 2(\tr x)(\tr y)
  \quad \text{for all $x,y \in \gl_n(k)$}.
 \]
 Because $\sll_n(k) \subideal \gl_n(k)$ it follows from Lemma~\ref{lem: restriction of the Killing form to an ideal} that the Killing form of $\sll_n(k)$ is given by
 \[
  \kappa_{\sll_n(k)}(x,y) = 2n \tr(xy) \quad \text{for all $x,y \in \sll_n(k)$}.
 \]
 In particular the Killing form of $\sll_n(k)$ is just a multiple of the trace form.
\end{expl}


\begin{lem}\label{lem: orthogonal ideals with respect to the killing form}
 Let $\g$ be a Lie algebra and $I_1, I_2 \subideal \g$ ideals with $\g = I_1 \oplus I_2$. Then $I_1 \perp I_2$ with respect to the Killing form $\kappa$ of $\g$. In particular it follows that for all $x,y \in \g$ with $x = x_1 + x_2$ and $y = y_1 + y_2$ with respect to $\g = I_1 \oplus I_2$
 \[
  \kappa(x,y) = \kappa_{I_1}(x_1, y_1) + \kappa_{I_2}(x_2, y_2)
 \]
\end{lem}
\begin{proof}
 Because $[I_1, I_2] \subseteq I_1 \cap I_2 = 0$ it follows that for every $z_1 \in I_1$ and $z_2 \in I_2$
 \[
  (\ad(z_1)\ad(z_2))(\g) = \ad(z_1)(\ad(z_2)(\g)) \subseteq \ad(z_1)(I_2) = 0.
 \]
 Therefore $\ad(z_1)\ad(z_2) = 0$ and in particular $\kappa(z_1,z_2) = \tr(\ad(z_1)\ad(z_2)) = 0$. From Lemma \ref{lem: restriction of the Killing form to an ideal} it further follows that
 \begin{align*}
  \kappa(x,y)
  &= \kappa(x_1, y_1) + \kappa(x_1, y_2) + \kappa(x_2, y_1) + \kappa(x_2, y_2) \\
  &= \kappa(x_1, y_1) + \kappa(x_2, y_2)
  = \kappa_{I_1}(x_1, y_1) + \kappa_{I_2}(x_2, y_2).
  \qedhere
 \end{align*}
\end{proof}





\subsection{The concrete Jordan decomposition}


\begin{defi}
 Let $V$ be an $n$-dimensional $k$-vector space and $x \in \End_k(V)$ (resp.\ $y \in \Mat_n(y)$). Then $x$ (resp.\ $y$) is called \emph{semisimple} if it is diagonalizable.
\end{defi}


\begin{rem}
 An endomorphism $x \in \End_k(V)$ as above is semisimple if and only if every $x$-invariant subspace of $V$ has a direct summand which is also $x$-invariant. (This depends on $k$ being algebraically closed.)
\end{rem}


\begin{thrm}\label{thrm: concrete Jordan decomposition}
 Let $V$ be a finite dimensional $k$-vector space and $x \in \End_k(V)$.
 \begin{enumerate}[leftmargin=*]
  \item
   There exist unique $x_s, x_n \in \End_k(V)$ satisfying the following properties:
   \begin{enumerate}
    \item
     $x = x_s + x_n$.
    \item
     $x_s$ is semisimple and $x_n$ is nilpotent.
    \item
     $x_s$ and $x_n$ commute.
   \end{enumerate}
  \item
   $x_s$ and $x_n$ are Polynomials in $x$ without constant term, i.e.\ there exist polynomials $P,Q \in k[T]$ such that $P(0) = Q(0) = 0$ and $x_s = P(x)$ and $x_n = P(x)$. In particular an endomorphism of $V$ commutes with $x$ if and only if it commutes with $x_s$ and $x_n$.
  \item
   If $A \subseteq B \subseteq V$ are linear subspaces with $x(B) \subseteq A$ then also $x_s(B) \subseteq A$ and $x_n(B) \subseteq A$.
 \end{enumerate}
\end{thrm}
\begin{proof}
 Let $\chi(T)$ be the characteristic polynomial of $x$ with $\chi(T) = \prod_{i=1}^n (T-\lambda_i)^{m_i}$ where $\lambda_i \neq \lambda_j$ for $i \neq j$. By the chinese reminder theorem the map
 \begin{equation}\label{eqn: use of chinese reminder theorem}
  \begin{aligned}
   k[T]/(\chi) &\longrightarrow \prod_{i=1}^n k[T]/((T-\lambda_i)^{m_i}),\\
   F + (\chi) &\longmapsto (F+((T-\lambda_1)^{m_1}), \dotsc, F+(((T-\lambda_n)^{m_n}))
  \end{aligned}
 \end{equation}
 is surjective. Thus there exists some polynomial $P \in k[T]$ with
 \begin{equation}\label{eqn: congruences of P}
  P(T) \equiv \lambda_i \mod (T-\lambda_i)^{m_i} \quad \text{for ever $i = 1, \dotsc, n$}.
 \end{equation}
 
 We can also assume that $P(0) = 0$. If $\lambda_i = 0$ for some $i$ then this follows directly from \eqref{eqn: congruences of P}. Otherwise the polynomials $(T-\lambda_1)^{m_1}$, \dots, $(T-\lambda_n)^{m_n}$, $T$ are pairwise coprime, so by replacing $\chi(T)$ with $\tilde{\chi}(T) \coloneqq \chi(T) T$ in \eqref{eqn: use of chinese reminder theorem} results in a polynomial $\tilde{P}$ which does not only satisfy \eqref{eqn: congruences of P} (with $P$ replaced by $\tilde{P}$) but also $\tilde{P} \bmod T = 0$.
 
 Now let $Q(T) \coloneqq T - P(T)$ as well as $x_s \coloneqq P(x)$ and $x_n \coloneqq Q(x)$. Then $x = x_s + x_n$ and $x_s$ and $x_n$ commute, as both are polynomials in $x$. For every $i = 1, \dotsc, n$ let
 \[
  V_i \coloneqq \ker (x-\lambda_i)^{m_i}
 \]
 be the generalized eigenspace of $x$ with respect to the eigenvalue $\lambda_i$. Is is known from linear algebra that $V = \bigoplus_{i=1}^n V_i$.
 
 It follows from \eqref{eqn: use of chinese reminder theorem} that for every $i = 1, \dotsc, n$ there exists some polynomial \mbox{$P_i \in k[T]$} with
 \[
  P(T) = \lambda_i + P_i(T) (T-\lambda_i)^{m_i},
 \]
 from which follows for every $v \in V_i$ that
 \[
  x_s(v)
  = (\lambda_i \id_V + P_i(x)(x-\lambda_i)^{m_i})(v)
  = \lambda_i v + P_i(x)(\underbrace{(x-\lambda_i)^{m_i}(v)}_{=0})
  = \lambda_i v.
 \]
 Hence $V_i$ is $x_s$-invariant with $x_s|_{V_i} = \lambda_i \id_{V_i}$ for every $i = 1, \dotsc, n$. As $V = \bigoplus_{i=1}^n V_i$ this shows that $x_s$ is semisimple and $V_i$ is precisely the eigenspace of $x_s$ to the eigenvalue $\lambda_i$.
 
 To see that $x_s$ is nilpotent notice that for every $i = 1, \dotsc, n$ and $v \in V_i$
 \[
  x_n(v) = x(v) - x_s(v) = x(v) - \lambda_i (v) = (x - \lambda_i \id_V)(v).
 \]
 Hence $V_i$ is $x_n$-invariant with $x_n|_{V_i} = x - \lambda_i \id_{V_i}$ for every $i = 1, \dotsc, n$. By definition of $V_i$ it follows that $x_n|_{V_i}$ is nilpotent for every $i = 1, \dotsc, n$. Because $V = \bigoplus_{i=1}^n V_i$ it follows that $x_n$ is nilpotent.
 
 This shows the existence of the claimed decomposition. For the uniqueness let $y_s, y_n \in \End_k(V)$ be any two endomorphisms with $x = y_s + y_n$ where $y_s$ is semisimple, $y_n$ is nilpotent and $y_s$ and $y_n$ commute. As $y_s$ and $y_n$ commute it follows that each of them commutes with $x = y_s + y_n$. Because $x_s$ and $x_n$ are polynomials in $x$ it follows from this that $y_s$ and $y_n$ both commute with $x_s$ and $x_n$. Hence $x_s$, $x_n$, $y_s$ and $y_n$ are pairwise commuting. In particular $x_s$ and $y_s$ are simultaneously diagonalizable which is why $x_s - y_s$ is also semisimple. As $x_n$ and $y_n$ commute and are both nilpotent it also follows that $y_n - x_n$ is nilpotent. But from $x_s + x_n = x = y_s + y_n$ it follows that
 \[
  \underbrace{x_s - y_s}_{\text{semisimple}} = \underbrace{y_n - x_n}_{\text{nilpotent}}.
 \]
 Hence $x_s - y_s = y_n - x_n = 0$.
 
 All other statements of the theorem directly follow from the construction of $x_s$ and $x_n$ and the fact that they are polynomials without constant term in $x$.
\end{proof}


\begin{rem}\label{rem: concrete Jordan decomposition for matrices}
 An analogeous statement of Theorem~\ref{thrm: concrete Jordan decomposition} can be shown for $\Mat_n(k)$ instead of $\End_k(V)$. More precisely: Every matrix $x \in \Mat_n(k)$ can be uniquely decomposed into $x = x_s + x_n$ such that $x_s$ is semisimple, $x_n$ is nilpotent and $x_s$ and $x_n$ commute. Both $x_s$ and $x_n$ are polynomials without constant term in $x$, so any matrix in $\Mat_n(k)$ commutes with $x$ if and only if it commutes with $x_s$ and $x_n$. If $A \subseteq B \subseteq k^n$ are linear subspaces such that $B$ is carried into $A$ by left multiplication with $x$, then the same goes for $x_s$ and $x_n$.
\end{rem}


\begin{defi}
 Let $V$ be an $n$-dimensional vector space and $x \in \End_k(V)$ (resp.\ $y \in \Mat_n(k)$). Then the decomposition $x = x_s + x_n$ from Theorem~\ref{thrm: concrete Jordan decomposition} (resp.\ the decomposition $y = y_s + y_n$ from Remark~\ref{rem: concrete Jordan decomposition for matrices}) is called the \emph{concrete Jordandecomposition} of $x$ (resp.\ $y$). The element $x_s$ (resp.\ $y_s$) is called the \emph{semisimple part} of $x$ (resp.\ $y$) and the element $x_n$ (resp.\ $y_n$) is called the \emph{nilpotent part} of $x$ (resp.\ $y$).
\end{defi}


\begin{defi}
 Let $\g$ be a finite dimensional Lie algebra. Then $x \in \g$ is called \emph{$\ad$-semisimple} if $\ad(x)$ is a semisimple endomorphism of $\g$.
\end{defi}


\begin{lem}\label{lem: ss and nilpotent implies ad-ss and ad-nilpotent}
 Let $\g \subseteq \gl(V)$ be a Lie subalgebra for some finite dimensional vector space $V$ (resp.\ $\g \subseteq \gl_n(k)$) and $x \in \g$.
 \begin{enumerate}
  \item
   If $x$ is semisimple then $x$ is also $\ad$-semisimple.
  \item
   If $x$ is nilpotent then $x$ is also $\ad$-nilpotent.
 \end{enumerate}
\end{lem}
\begin{proof}
 We only show the case $\g \subseteq \gl(V)$, the proof for the case $\g \subseteq \gl_n(k)$ being essentially the same.
 \begin{enumerate}[leftmargin=*]
  \item
   Let $e_1$, \dots, $e_n$ be a basis of $\g$ consisting of eigenvectors of $x$, where $e_i$ belongs to the eigenvalue $\lambda_i \in k$. Then for all \mbox{$i,j = 1, \dotsc, n$} let $E_{ij} \in \End_k(\g)$ be defined by
   \[
    E_{ij}(e_k) = \delta_{jk} e_i \quad \text{for every $k = 1, \dotsc, n$}.
   \]
   Then $(E_{ij})_{i,j=1,\dotsc,n}$ is a basis of $\End_k(\g)$. For all $i,j,k = 1, \dotsc, n$ it follows that
   \begin{align*}
    [x,E_{ij}](e_k)
    &= (x E_{ij} - E_{ij} x)(e_k)
    = x(E_{ij}(e_k)) - E_{ij}(x(e_k)) \\
    &= \delta_{jk} x(e_i) - \lambda_k E_{ij}(e_k)
    = \delta_{jk} \lambda_i e_i - \delta_{jk} \lambda_k e_i \\
    &= (\lambda_i - \lambda_j) \delta_{jk} e_i
    = (\lambda_i - \lambda_j) E_{ij} e_k.
   \end{align*}
   It follows that
   \[
    \ad_{\gl(V)}(x)(E_{ij}) = [x,E_{ij}] = (\lambda_i - \lambda_j) E_{ij}
    \quad \text{for all $i,j = 1, \dotsc, n$},
   \]
   so $\ad_{\gl(V)}$ is semisimple and therefore also the restriction $\ad_\g(x) = \ad_{\gl(V)}(x)|_{\g}$.
   
  \item
   In Lemma~\ref{lem: nilpotent implies ad-nilpotent} it was already shown that $\ad_{\gl(V)}(x)$ is nilpotent. From this it follows that the restriction $\ad_\g(x) = \ad_{\gl(V)}(x)|_{\g}$ is also nilpotent.
  \qedhere
 \end{enumerate}
\end{proof}


\begin{cor}\label{cor: concrete Jordan decomposition compatible with adjoint representation}
 Let $\g \subseteq \gl(V)$ be a Lie subalgebra for a finite dimensional vector space $V$ (resp.\ $\g = \gl_n(k)$). If $x \in \g$ has the Jordan decomposition $x = x_s + x_n$ then $\ad(x) = \ad(x_s) + \ad(x_n)$ is the Jordan decomposition of $\ad(x)$.
\end{cor}
\begin{proof}
 As $x_s$ is semisimple the same goes for $\ad(x_s)$ and as $x_n$ is nilpotent the same goes for $\ad(x_n)$, each following from Lemma~\ref{lem: ss and nilpotent implies ad-ss and ad-nilpotent}. As $x_s$ and $x_n$ commute so do $\ad(x_n)$ and $\ad(x_s)$ because $\ad$ is a homomorphism of Lie algebras.
\end{proof}





\subsection{Cartanâ€™s Criterion}


\begin{lem}\label{lem: Cartan's criterion technical lemma}
 Let $V$ be a finite dimensional $k$-vector space. Let $A \subseteq B \subseteq \gl(V)$ be linear subspaces and let
 \[
  T \coloneqq \{z \in \gl(V) \mid \ad(z)(B) \subseteq A\}.
 \]
 If $x \in T$ and $\tr(xz) = 0$ for every $z \in T$ then $x$ is nilpotent.
\end{lem}
\begin{proof}
 Let $x = x_s + x_n$ be the concrete Jordan decompositon of $x$. Then the concrete Jordan decomposition of $\ad(x)$ is given by $\ad(x) = \ad(x_s) + \ad(x_n)$ by Corollary~\ref{cor: concrete Jordan decomposition compatible with adjoint representation}. As $\ad(x)(B) \subseteq A$ it follows from the properties of the concrete Jordan decomposition (see Theorem~\ref{thrm: concrete Jordan decomposition}) that also $\ad(x_s)(B) = \ad(x)_s(B) \subseteq A$. Hence $x_s \in T$.
 
 Let $(v_1, \dotsc, v_n)$ be an ordered basis of $V$ with respect to which $x$ is in Jordan normal form. Then with respect to this basis $x_s$ is diagonal and $x_n$ is strictly upper triangular. Let $\lambda_i \in k$ with $x_s(v_i) = \lambda_i v_i$ for every $i = 1, \dotsc, n$ and set
 \[
  E \coloneqq \vspan_\Q(\lambda_1, \dotsc, \lambda_n) \subseteq k.
 \]
 To show that $x$ is nilpotent it sufficies to show that $x_s = 0$, which is equivalent to $\lambda_i = 0$ for every $i = 1, \dotsc, n$. As this is the same as $E = 0$ it is enough to show that $f = 0$ for every $\Q$-linear map $f \colon E \to \Q$. For the rest of the proof we fix such an $f$. Let $z \colon V \to V$ be defined by
 \[
  z(v_i) \coloneqq f(\lambda_i) v_i \quad \text{for every $i = 1, \dotsc, n$}.
 \]
 
 \begin{claim*}
  $z \in T$.
 \end{claim*}
 \begin{proof}
  For all $i,j = 1, \dotsc, n$ let $e_{ij} \in \gl(V)$ with
  \[
   e_{ij}(v_k) = \delta_{jk} v_i \quad \text{for every $k = 1, \dotsc, n$}.
  \]
  Then $(e_{ij})_{i,j = 1, \dotsc, n}$ is a $k$-basis of $\gl(V)$. As already seen in the proof of Corollary~\ref{cor: concrete Jordan decomposition compatible with adjoint representation} this is a basis of eigenvectors of $\ad(x_s)$ where $e_{ij}$ is an eigenvector of $\ad(x_s)$ with respect to the eigenvalue $\lambda_i - \lambda_j$. Because $(v_1, \dotsc, v_n)$ is also a basis of $V$ consisting of eigenvalues of $z$, where $v_i$ belongs to the eigenvalue $f(\lambda_i)$, it follows in the same way, that $e_{ij}$ is an eigenvector of $\ad(z)$ with respect to the eigenvalue
  \[
   \mu_{ij} \coloneqq f(\lambda_i) - f(\lambda_j) = f(\lambda_i - \lambda_j)
  \]
  for all $i,j = 1, \dotsc, n$. In particular it follows that if $e_{ij}$ and $e_{i'j'}$ have the same eigenvalue with respect to $\ad(x_s)$, i.e.\ if $\lambda_i - \lambda_j = \lambda_{i'} - \lambda_{j'}$ then the same goes holds with respect to $\ad(z)$. Hence if $y \in \gl(V)$ is an eigenvector of $\ad(x_s)$ with respect to the eigenvalue $\lambda$ then $y$ is an eigenvector of $\ad(z)$ with respect to the eigenvalue $f(y)$.
  
  As $\ad(x)(B) \subseteq A \subseteq B$ there exists a decomposition $B = A \oplus N$ into linear subspaces with $A$ being $\ad(x)$-invariant and thus decomposing into $\ad(x)$-eigenspaces and $N \subseteq \ker \ad(x)$. Then by the previous observations it follows that $A$ also decomposes into $\ad(z)$-eigenspaces and that $\ad(z)(N) = 0$. Hence $\ad(z)(B) \subseteq A$ and thus $z \in T$.
 \end{proof}
 
 Now \mbox{$\tr(x z) = \tr(x_s z) + \tr(x_n z)$}. As $x_s$ and $z$ are both diagonal with respect to the basis $(v_1, \dotsc, v_n)$ it follows that $\tr(x_s z) = \sum_{i=1}^n f(\lambda_i) \lambda_i$, and as $x_n z$ is also stricly upper triangular it also follows that $\tr(x_n z) = 0$. Together with $z \in T$ this results in
 \[
  0 = \tr(xz) = \sum_{i=1}^n f(\lambda_i) \lambda_i
 \]
 Because $f(\lambda_i) \in \Q$ and $\lambda_i \in E$ for every $i = 1, \dotsc, n$ applying $f$ to this equation results in
 \[
  0 = \sum_{i=1}^n f(\lambda_i)^2.
 \]
 It follows that $f(\lambda_i) = 0$ for every $i = 1, \dotsc, n$ and thus $f = 0$.
\end{proof}


\begin{rem}
 The proof of Lemma~\ref{lem: Cartan's criterion technical lemma} was not actually given in the lecture itself and proving it was an exercise on the third exercise sheet. The proof given above is the one I came up with based on some hints given on the exercise sheet. At some point it will proberly be merged with the proof given in \cite[\S 4.3]{Humphreys}.
 
 In the lecture a proof was given for the special case $k = \Cbb$. But since I have some trouble understanding the details it is not included here (yet).
\end{rem}


\begin{lem}[Cartanâ€™s criterion for $\gl(V)$]
 Let $V$ be a finite dimensional $k$-vector space and $\g \subseteq \gl(V)$ a Lie subalgebra. Then $\g$ is solvable if and only if $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$.
\end{lem}
\begin{proof}
 Suppose that $\g$ is solvable. Then by Lieâ€™s theorem there exists a basis of $V$ with respect to which $\g$ is represented by upper triangular matrices. Then $[\g,\g]$ is represented by strictly upper triangular matrices, which is why $xy$ is also represented by a stricly upper triangular matrix for every $x \in \g$ and $y \in [\g,\g]$. Hence $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$.
 
 Now suppose that $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$. Set $A \coloneqq [\g,\g]$, $B \coloneqq \g$ and
 \[
  T \coloneqq \{x \in \gl(V) \mid \ad(x)(B) \subseteq A\}.
 \]
 Let $x \in [\g,\g] \subseteq T$ and $z \in T$. Then $[z,\g] = [z,B] \subseteq A = [\g,\g]$. Writing $x$ as $x = \sum_{i=1}^n [a_i, b_i]$ with $a_i, b_i \in \g$ for every $i = 1, \dotsc, n$ it follows that
 \begin{align*}
  \tr(xz)
  &= \sum_{i=1}^n \tr([a_i, b_i] z)
  = \sum_{i=1}^n \kappa_{\gl(V)}([a_i, b_i], z) \\
  &= \sum_{i=1}^n \kappa_{\gl(V)}(a_i, [b_i, z])
  = \sum_{i=1}^n \tr(a_i \underbrace{[b_i, z]}_{\in [\g,\g]})
  = 0,
 \end{align*}
 where the last step uses the asumption. It follows from Lemma~\ref{lem: Cartan's criterion technical lemma} that $x$ is nilpotent. Because $[\g,\g]$ consists of nilpotent elements there exists a basis of $V$ with respect to which $[\g,\g]$ is represented by stricly upper triangular matrices. Hence $[\g,\g]$ is nilpotent und $\g$ therefore solvable.
\end{proof}


\begin{thrm}[Cartanâ€™s criterion for solvability]
 Let $\g$ be a finite dimensional Lie algebra. Then $\g$ is solvable if and only if
 \[
  \kappa(x,y) = 0 \quad \text{for every $x \in \g$ and $y \in [\g,\g]$}.
 \]
\end{thrm}
\begin{proof}
 Because $Z(\g)$ is a solvable ideal in $\g$ it follows that $\g$ is solvable if and only if $\g/Z(\g) \cong \ad \g \subseteq \gl(\g)$ is solvable. By Cartanâ€™s criterion for $\gl(\g)$ this is the case if and only if
 \[
  \tr(xy) = 0 \quad \text{for every $x \in \ad \g$ and $y \in [\ad(\g), \ad(\g)]$}.
 \]
 Because $[\ad(\g), \ad(\g)] = \ad([\g,\g])$ and $\tr(\ad(x)\ad(y)) = \kappa(x,y)$ for all $x,y \in \g$ this is equivalent to
 \[
  \kappa(x,y) = 0 \quad \text{for every $x \in \g$ and $y \in [\g,\g]$}.
  \qedhere
 \]
\end{proof}


\begin{cor}\label{cor: rad kappa is a solvable ideal}
 Let $\g$ be a finite dimensional Lie algebra and $\kappa$ the Killing form of $\g$. Then $\rad \kappa$ is a solvable ideal of $\g$. In particular $\rad \kappa \subseteq \rad \g$.
\end{cor}
\begin{proof}
 Lemma~\ref{lem: orthogonal complement of an ideal is again an ideal} already showed that $\rad \kappa$ is an ideal in $\g$. From Lemma~\ref{lem: restriction of the Killing form to an ideal} and the definition of $\rad \kappa$ it follows that 
 \[
  \kappa_{\rad \g}(x,y) = \kappa(x,y) = 0 \quad \text{for all $x,y \in \rad \kappa$}.
 \]
 Hence by Cartanâ€™s criterion $\rad \kappa$ is solvable.
\end{proof}