\section{Nilpotent and solvable Lie algebras}





\subsection{Definition und some properties}


\begin{defi}
 Let $A$ be an associative $k$-algebra. An element $a \in A$ is called \emph{nilpotent} if $a^n = 0$ for some $n \geq 1$. Given a Lie algebra $\g$ an element $x \in \g$ is called \emph{$\ad$-nilpotent} if $\ad(x) \in \End_k(\g)$ is nilpotent.
\end{defi}


\begin{lem}
 If $A$ is an associative $k$-algebra and $x \in A$ nilpotent then $x$ is also $\ad$-nilpotent.
\end{lem}
\begin{proof}
 Let $\lambda_x \colon A \to A, a \mapsto xa$ and $\rho_x \colon A \to A, a \mapsto ax$. Because $x$ is nilpotent both $\lambda_x$ and $\rho_x$ are nilpotent. Because $A$ is associative $\lambda_x$ and $\rho_x$ commute. Hence $\ad(x) = \lambda_x - \rho_x$ is the sum of two commuting, nilpotent endomorphisms, and therefore also nilpotent.
\end{proof}


\begin{defi}
 Let $g$ be a Lie algebra. Define $\g^0 \coloneqq \g$ and $\g^{i+1} \coloneqq [\g,\g^i]$ for all $i \in \N$. Then
 \[
  \g = \g^0 \supseteq \g^1 \supseteq \g^2 \supseteq \dots
 \]
 is called the \emph{central series} of $\g$. Also define $\g^{(0)} \coloneqq \g$ and $\g^{(i+1)} \coloneqq [\g^{(i)},\g^{(i)}]$ for all $i \in \N$. Then
 \[
  \g^{(0)} \supseteq \g^{(1)} \supseteq \g^{(2)} \supseteq \dots
 \]
 is called the \emph{derived series} of $\g$. $\g$ is called \emph{nilpotent} if $\g^i = 0$ for some $i$ and \emph{solvable} if $g^{(i)} = 0$ for some $i$.
\end{defi}


It is clear that every nilpotent Lie algebra is also solvable.


\begin{expls}
 \begin{enumerate}[leftmargin=*]
  \item
   The upper triangular matrices $\tl_n(k)$ are solvable. But they are not nilpotent.
  \item
   The strictly upper triangular matrices $\nl_n(k)$ not only solvable but also nilpotent.
  \item
   If $n \geq 2$ then $\sll_2(\Cbb)$ is simple and therefore $[\sll_n(\Cbb),\sll_n(\Cbb)] = \sll_n(\Cbb)$. Since $[\gl_n(\Cbb),\gl_n(\Cbb)] = \sll_n(\Cbb)$ it follows that $\gl_n(\Cbb)$ is not solvable.
  \item
   If $\g$ is abelian then $\g$ is nilpotent.
  \item
   A \emph{Heisenberg Lie algebra} consists of a real vector space with basis $P_1, \dotsc, P_n$, $Q_1, \dotsc, Q_n$, $C$ together with the Lie bracket satisfying the following conditions:
   \[
    [P_i, P_j] = [Q_i, Q_j] = [P_i, C] = [Q_i, C] = 0
    \quad \text{and} \quad
    [P_i, Q_j] = \delta_{ij} C.
   \]
   This defines a nilpotent Lie algebra.
 \end{enumerate}
\end{expls}


\begin{prop}\label{prop: properties of solvable and nilpotent}
 Let $\g$ be a Lie algebra.
 \begin{enumerate}[leftmargin=*]
  \item
   If $\h$ is a Lie algebra and $f \colon \g \to \h$ a Lie algebras homomorphism then $f(\g)^i = f(\g^i)$ and $f(\g)^{(i)} = f(\g^{(i)})$ for all $i \geq 0$.
  \item
   If $\g$ is nilpotent (resp.\ solvable) then any Lie subalgebra $\h \subseteq \g$ and any quotient of $\g$ (by an ideal $I$) is nilpotent (resp.\ nilpotent).
  \item
   If $I \subideal \g$ with $I \subseteq Z(\g)$ and $\g/I$ is nilpotent then $\g$ is nilpotent.
  \item
   If $\g \neq 0$ is nilpotent then $Z(\g) \neq 0$.
  \item
   If $\g$ is nilpotent and $x \in \g$ then $x$ is $\ad$-nilpotent.
  \item
   If $I \subideal \g$ then $I^i$ and $I^{(i)}$ are ideals inside $\g$ for all $i \geq 0$.
 \end{enumerate}
\end{prop}
\begin{proof}
 \begin{enumerate}[leftmargin=*]
  \item
   It suficies to show that for any two subsets $X, Y \subseteq \g$
   \[
    f([X,Y])= [f(X),f(Y)]
   \]
   the statement then follows inductively. It holds because $f$ is a Lie algebra homomorphism and therefore
   \begin{align*}
    f([X,Y])
    &= f(\vspan_k \{[x, y] \mid x \in X, y \in Y\}) \\
    &= \vspan_k \{f([x, y]) \mid x \in X, y \in Y\} \\
    &= \vspan_k \{[f(x),f(y)] \mid x \in X, y \in Y\} \\
    &= \vspan_k \{[x',y'] \mid x' \in f(X), y' \in f(Y)\} \\
    &= [f(X),f(Y)].
   \end{align*}
  \item
   The statement about subalgebras follows from $\h^i \subseteq \g^i$ and $\h^{(i)} \subseteq \g^{(i)}$ for all $i \in \N$. The statement about quotient follow by using the canonical projection $\pi \colon \g \to \g/I$. Because $\pi$ is a Lie algebra homomorphism it follows that
   \[
    (\g/I)^i = \pi(\g)^i = \pi(\g^i) = 0
   \]
   for $i$ big enough. For solvable $\g$ the corresponding statements follow in the same way.
  \item
   Let $\pi \colon \g \to \g/I$ be the canonical projection. Because $\g/I$ is nilpotent there exists some $i \geq 0$ with $(\g/I)^i = 0$ and therefore
   \[
    0 = (\g/I)^i = \pi(\g)^i = \pi(\g^i).
   \]
   Thus $\g^i \subseteq I \subseteq Z(G)$ and hence $\g^{i+1} = 0$.
  \item
   Let $i \in \N$ be minimal with $\g^i \neq 0$ but $\g^{i+1} = 0$. Then $\g^i \subseteq Z(\g)$ and thus $Z(\g) \neq 0$.
  \item
   Since $\g$ is nilpotent there exists some $i \in \N$ with $\g^i = 0$. Then
   \[
    (\ad(x))^i(\g) \subseteq \g^i = 0,
   \]
   so $(\ad(x))^i = 0$.
  \item
   This follows inductively by using that $[I,J]$ is an ideal inside $\g$ for any $I,J \subideal \g$.
 \end{enumerate}
\end{proof}





\subsection{Engelâ€™s theorem}


From now on \emph{all} fields over which we work will be assumed to be algebraically closed, unless otherwise specified.


If $V$ is an $n$-dimensional vector space over $k$ and $x \in \End_k(V)$ a nilpotent endomorphism then $0$ is the only eigenvalue of $x$ (and occurs with multiplicity $n$) (here it is used that $k$ is algebraically closed). Hence there exists an eigenvector $v \in V$, $v \neq 0$ with $x(v) = 0$. The following proposition generalizes this observations for linear Lie algebras consisting of nilpotent endomorphisms.


\begin{prop}\label{prop: common eigenvector for nilpotent Lie algebras}
 Let $V \neq 0$ be a finite dimensional vector space and $\g \subseteq \gl(V)$ a Lie subalgebra such that every $x \in \g$ in nilpotent. Then there exists $v \in V$ with $v \neq 0$ and $x(v) = 0$ for every $x \in \g$, i.e.\ $v$ is a common eigenvector of all $x \in \g$ (all of which are nilpotent and thus have $0$ as their only eigenvalue).
\end{prop}
\begin{proof}
 The statement can be then shown by induction over $\dim \g$. For $\dim \g = 0$ the statement follows from $\g = 0$ and for $\dim \g = 1$ the statement follows as previously discussed from $\g = k x$ with $x$ being a nilpotent endomorphism of $V$.
 
 So let $\dim \g \geq 2$ and suppose that the statement holds for all smaller dimensions. Let $I \subseteq \g$ be a maximal proper Lie subalgebra (such a subalgebra exists because it is precisely one of maximal dimension strictly smaller than $n$). Any $x \in \g$ with $x \neq 0$ spans a one-dimensional subalgebra $k x$ of $\g$; because $\dim \g \geq 2$ it is a proper one. This show that $I \neq 0$. It turns out that $I$ is already in ideal in $\g$:
 
 By assumption $\g$ consists of nilpotent endomorphisms and therefore of $\ad$-nilpotent elemenents. In particular every $x \in I$ acts nilpotent on $\g$ via $\ad(x)$ with $I$ being an $\ad(x)$-invariant linear subspace. Therefore every $x \in I$ acts on the quotient vector space $\g/I$ by an induced nilpotent endomorphism
 \[
  \overline{\ad}(x) \colon \g/I \to \g/I, \quad y + I \mapsto \ad(x)(y) + I = [x,y] + I.
 \]
 As the map $\overline{\ad} \colon I \to \gl(\g/I)$ is an homomorphism of Lie algebras (because $\ad$ is) the image $\{\overline{\ad}(x) \mid x \in I\} \subseteq \gl(\g/I)$ is an Lie subalgebra, consisting of nilpotent endomorphisms. From $I \neq 0$ it follows that $\dim \g/I < \dim \g$, so by induction assumption there exists some $y \in \g$ with $\overline{\ad}(x)(y+I) = 0$ for every $x \in I$ and $y+I \neq 0+I$. Hence $[x,y] \in I$ for every $x \in I$ but $y \neq I$. Hence $y \in N_\g(I)$ with $y \neq I$, so $I$ is properly contained in its normalizer. As $I$ is a maximal proper subalgebra of $\g$ it follows that $N_\g(I) = \g$, so $I$ is an ideal. It is even one of codimension $1$:
 
 If $I$ had not codimension $1$ then $\dim \g/I > 1$. Then $\g/I$ contains a one-dimensional proper subalgebra $L$ (as seen above), and the preimage $\pi^{-1}(J)$ under the canonical projection $\pi \colon \g \to \g/I$ is then a proper subalgebra of $\g$ properly containing $I$, which contradicts the maximality of $I$. Hence $I$ has codimension $1$.
 
 As $I \subseteq \g$ has codimension $1$ there exists some $y \in \g$ with $g = I \oplus ky$ (as vector spaces). Because $\dim I < \dim \g$ it follows from the induction assumption that
 \[
  U \coloneqq \{v \in V \mid \text{$x(v) = 0$ for every $x \in I$}\} = \bigcap_{x \in I} \ker x
 \]
 is a nonzero linear subspace of $V$. It sufficies to show that $U$ is $y$-invariant: Then there exists some eigenvector $u \in U$ of $y$ for which necessarily $y(u) = 0$. If $u\in U$ then $[x,y] \in I$ for every $x \in I$ because $I \subideal \g$ and therefore
 \[
  x(y(u)) = [x,y](u) - y(x(u)) = 0 - y(0) = 0.
 \]
 Hence $y(u) \in U$, so $U$ is $y$-invariant.
\end{proof}


\begin{prop}\label{prop: stuff for Engels theorem}
 Let $V$ be a finite dimensional vector space with $n = \dim V$ and $\g \subseteq \gl(V)$ a Lie subalgebra. Then the following are equivalent:
 \begin{enumerate}
  \item\label{enum: engels g consists of nilpotent endomorphisms}
   $\g$ consists of nilpotent endomorphisms.
  \item\label{enum: engels there exists a complete flag shifted by g}
   There exists a complete flag of $V$
   \[
    V = V_n \supsetneq V_{n-1} \supsetneq V_{n-2} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0 = 0,
   \]
   with $x(V_i) \subseteq V_{i-1}$ for every $i = 1, \dotsc, n$.
  \item\label{enum: engels represented by strictly upper triangular matrices}
   There exists a basis of $V$ with respect to which every $x \in \g$ is represented by an strictly upper triangular matrix.
 \end{enumerate}
\end{prop}
\begin{proof}
 The implication \ref{enum: engels g consists of nilpotent endomorphisms} $\Rightarrow$ \ref{enum: engels there exists a complete flag shifted by g} can be shown by induction over $\dim V$. For $\dim V = 1$ set $V_0 \coloneqq 0$ and $V_1 \coloneqq V$. By assumption every $x \in \g$ acts nilpotent on $V$, so $x(V) = 0$ because $V$ is one-dimensional. Thus $V = V_1 \supsetneq V_0 = 0$ is a complete flag for $V$ satisfying the condititons.
   
 Now let $\dim V = n \geq 2$ and suppose the statement holds for all smaller dimensions. Let $v \in V$, $v \neq 0$ with $x(v) = 0$ for every $x \in \g$ and $W \coloneqq V / kv$. Every $x \in \g$ induces an endomorphism
 \[
  \overline{x} \colon W \to W, \quad v + kv \mapsto x(v) + kv.
 \]
 By induction assumption exists a complete flag
 \[
  W = W_{n-1} \supsetneq W_{n-2} \supsetneq W_{n-3} \supsetneq \dotsb \supsetneq W_1 \supsetneq W_0 = 0
 \]
 with $\overline{x}(W_i) \subseteq W_{i-1}$ for every $x \in \g$ and $i = 1, \dotsc, n-1$. By setting $V_i \coloneqq \pi^{-1}(W_{i-1})$ for every $i = 1, \dotsc, n$ and $V_0 = 0$ it follows that
 \[
  V = V_n \supsetneq V_{n-1} \supsetneq V_{n-2} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0 = 0,
 \]
 is a complete flag of $V$. On the one hand $x(V_1) = x(kv) = 0 = V_0$ for every $x \in \g$ and on the other hand
 \[
  \pi(x(V_i))
  = \overline{x}(\pi(V_i))
  = \overline{x}(W_{i-1})
  \subseteq W_{i-2}
 \]
 and therefore $x(V_i) \subseteq \pi^{-1}(W_{i-2}) = V_{i-1}$ for every $i = 2, \dotsc, n$.
 
 The implications \ref{enum: engels there exists a complete flag shifted by g} $\Rightarrow$ \ref{enum: engels represented by strictly upper triangular matrices} and \ref{enum: engels represented by strictly upper triangular matrices} $\Rightarrow$ \ref{enum: engels g consists of nilpotent endomorphisms} are basic facts from linear algebra.
\end{proof}





\begin{thrm}[Engel]
 Let $\g$ be a finite dimensional Lie algebra. Then $\g$ is nilpotent if and only if all its elements are $\ad$-nilpotent.
\end{thrm}
\begin{proof}
 If $\g$ is nilpotent then there exists some $i \in \N$ with $\g^i = 0$, from which is follows from $\ad(x)^i(y) \in \g^i$ for every $x,y \in \g$ that $\ad(x)^i = 0$ for every $x \in \g$, hence every $x \in \g$ is $\ad$-nilpotent.
 
 On the other hand suppose that $\g$ consists of $\ad$-nilpotent elemenents. If $\g = Z(\g)$ then $\g$ is abelian and hence nilpotent, so it sufficies to show the statement under the additional assumption that $Z(\g) \subsetneq \g$. Because $\g/Z(\g) \cong \ad \g$ is a Lie subalgebra of $\gl(\g)$ consisting of nilpotent Elemenents it follows from Proposition \ref{prop: stuff for Engels theorem} that $\g/Z(\g)$ is isomorphic to a Lie subalgebra of $\nl_n(k)$ for $n = \dim \g/Z(\g) \geq 1$. Because $\nl_n(k)$ is nilpotent the same goes for $\g/Z(\g)$ as seen in Proposition \ref{prop: properties of solvable and nilpotent}.
\end{proof}





\subsection{Lieâ€™s theorem}


From now on we will not only require \emph{every} field $k$ we work with to be algebraically closed, but also to be of characteristic $0$. Unless otherwise stated this holds up to the last page (page \pageref{LastPage}) of this text. In particular all Lie algebras and vector spaces will be assumed to have such a field as their ground field, even if not explicitely stated.


\begin{defi}
 Let $V$ be a representation of a Lie algebra $\g$. For $\lambda \in \g^*$ the linear subspace
 \[
  V_\lambda \coloneqq \{v \in V \mid \text{$x.v = \lambda x$ for every $x \in \g$}\}
 \]
 is called the \emph{$\g$-weight space} of $V$ with \emph{weight} $\lambda$.
\end{defi}



\begin{lem}[Invariance Lemma]
 Let $V$ be a finite dimensional representation of a Lie algebra $\g$ and $I \subideal \g$ an ideal. Then $V$ is also a representation of $I$ by restriction of the action of $\g$ on $V$ to $I$. For $\lambda \in I^*$ let $V_\lambda$ be the $I$-weight space of $V$ with weight $\lambda$. Then $V_\lambda$ is a subrepresentation of $\g$.
\end{lem}
\begin{proof}
 For $v \in V$ and $x_1, \dotsc, x_n \in \g$ we will write
 \[
  x_1 \dotsm x_n v \coloneqq x_1.(\dotsc.(x_n.v)).
 \]
 If $V_\lambda = 0$ the statement is clear, so for this proof we fix some $\lambda \in I^*$ with $V_\lambda \neq 0$.
 
 That $V_\lambda$ is a subrepresentation of $\g$ means that $yv \in V_\lambda$ for every $y \in \g$ and $v \in V_\lambda$, which is equivalent to $xyv = \lambda(x)yv$ for every $x \in I$, $y \in \g$ and $v \in V_\lambda$. Because
 \[
  xyv = [x,y]v + yxv = \lambda([x,y])v + \lambda(x)yv \quad \text{for every $x \in I$, $y \in \g$ and $v \in V_\lambda$}
 \]
 this is equivalent to $\lambda([x,y]) = 0$ for every $x \in I$ and $y \in \g$.
 
 Until further notice we fix some $y \in \g$ and $v \in V_\lambda$ with $v \neq 0$. As $V$ is finite dimensional there exists some maximal $n \geq 1$ such that $v$, $yv$, \dots, $y^n v$ are linearly independent. Let
 \[
  W_i \coloneqq \vspan_k(v, yv, \dotsc, y^i v) \quad \text{for every $i = 0, \dotsc, n$}.
 \]
 Because $v$, $yv$, \dots, $y^n v$, $y^{n+1} v$ are linearly dependent it follows that $W_n$ is invariant under the action of $y$.
 
 \begin{claim*}
  The linear subspace $W_i$ is for every $i = 0, \dotsc, n$ a subrepresentation of $I$. With respect to the basis $w$, $yw$, \dots, $y^i w$ of $W_i$ the action of $x \in I$ is represented by an upper triangular matrix where every diagonal entry is $\lambda(x)$.
 \end{claim*}
 \begin{proof}
  The claim can be proven by induction over $i$. As $W_0 = kv$ with $xv = \lambda(x)v$ for every $x \in I$ the claim holds for $i = 0$. Suppose that $i < n$ and that the claim holds for $W_0$, \dots, $W_i$. If $x \in I$ then also $[x,y] \in I$ and therefore
  \[
   x y^{i+1} v
   = \underbrace{[x,y] y^i v}_{\mathclap{\substack{\in W_i \\ \text{by induction}}}} + y x y^i v
   \equiv y x y^i v
   \mod W_i.
  \]
  By induction it is not only $x y^i v \in W_i$ but also $x y^i v + W_{i-1} = \lambda(x) y^i v + W_{i-1}$. Therefore
  \[
   y x y^i v \equiv \lambda(x) y^{i+1} v \mod W_i.
  \]
  This shows the claim for $W_{i+1}$.
 \end{proof}
 
 Let $x \in I$. As $[x,y] \in I$ it follows from the previous claim that the $(n+1)$-dimensional linear subspace $W_n$ is invariant under the action of $[x,y]$, which is given by an endomorphism $\phi_{[x,y]} \in \End_k(W_n)$, and that $\phi_{[x,y]}$ is represented by an upper triangular matrix for which all diagonal entries are $\lambda([x,y])$. It follows that in particular
 \begin{equation}\label{eqn: invariance lemma zero trace}
  \tr \phi_{[x,y]} = (n+1) \lambda([x,y])
 \end{equation}
 On the other hand $W_n$ is invariant under the action of both $x$ (by the claim) and $y$, which act by endomorphisms $\phi_x, \phi_y \in \End_k(V)$. As $V$ is a representation of the Lie algebra $\g$ it follows that $\phi_{[x,y]} = [\phi_x, \phi_y]$ and thus $\tr \phi_{[x,y]} = 0$. Together with \eqref{eqn: invariance lemma zero trace} it follows that $\lambda([x,y]) = 0$.
\end{proof}


As a generalization of Proposition \ref{prop: common eigenvector for nilpotent Lie algebras} we have the following result about solvable linear Lie algebras.


\begin{thrm}[Lie]\label{thrm: Lieâ€™s theorem}
 Let $V \neq 0$ be a finite dimensional $k$-vector space and $\g \subseteq \gl(V)$ a solvable Lie subalgebra. Then there exists a common eigenvector for $\g$, i.e.\ some $v \in V$, $v \neq 0$ with $x(v) \in k v$ for every $x \in \g$.
\end{thrm}
\begin{proof}
 The statement can be shown by induction over $\dim \g$. If $\dim \g = 0$ then $\g = 0$ and any $v \in V$ with $v \neq 0$ does the job. If $\dim \g = 1$ then $\g = k x$ for some $x \in \gl(V)$ with $x \neq 0$. Then any eigenvector of $x$ does the job (since $k$ is assumed to be algebraically closed and $V \neq 0$ such an eigenvector does exist).
 
 Suppose that $\dim \g = n \geq 2$ and the statement holds for every smaller dimension. Similarly to the proof of Proposition \ref{prop: common eigenvector for nilpotent Lie algebras} we will split this proof into four consecutive parts:
 \begin{enumerate}
  \item
   Finding an ideal $I \subideal \g$ of codimension $1$.
  \item
   Finding common eigenvectors for $I$ by induction.
  \item
   Showing that $\g$ stabilizes as nonzero subspace $U \subseteq V$ of such eigenvectors.
  \item
   Writing $\g = I \oplus k y$ (as vector spaces) and finding an eigenvector of $y$ in $U$.
 \end{enumerate}
 
 For the first step notice that $\g$ is nonzero but solvable, so $[\g,\g] \subideal \g$ is a proper ideal. Hence $\g/[\g,\g]$ is nonzero abelian Lie algebra. Therefore there exists a linear subspace $J \subseteq \g/[\g,\g]$ of codimension $1$ and $J$ is an ideal in $\g/[\g,\g]$. Hence the preimage $I = \pi^{-1}(J)$ for the canonical projection $\g \to \g/[\g,\g], x \mapsto x + [\g,\g]$ is an ideal in $\g$ of codimension $1$.
 
 For the second step notice that because $\g$ is solvable the same goes for $I$. So by induction hypothesis there exists a common eigenvector for $I$. Hence there exists some $\lambda \in I^*$ with $U \coloneqq V_\lambda \neq 0$, where we view $V$ as a representation of $\g$ via $x.v = x(v)$ for every $x \in \g$ and $v \in V$.
 
 The third step follows directly from the invariance lemma.
 
 For the fourth step let $y \in \g$ with $\g = I \oplus k y$ (as vector spaces). Since $\g$ stabilizes $U$ this holds in particular for $y$. As $U \neq 0$ it follows that there exists some eigenvector of $y$ inside of $U$, which is then a common eigenvector for $\g$.
\end{proof}


\begin{rem}
 The proof for Lieâ€™s theorem given in the lecture is basically a less structured version of the one in \cite[\S 4.1]{Humphreys}, from where we took the idea of breaking down the proof into four steps to emphasize the similarities with the proof of Proposition \ref{prop: common eigenvector for nilpotent Lie algebras} (which we found very useful for understanding the structure of the previous proof).
\end{rem}


\begin{prop}\label{prop: common eigenvector for solvable Lie algebras}
 Let $V \neq 0$ be a finite dimensional $k$-vector space with $n = \dim V$ and $\g \subseteq \gl(V)$ a Lie subalgebra. Then the following are equivalent:
 \begin{enumerate}[leftmargin=*]
  \item
   $\g$ is solvable.
  \item
   $\g$ stabilizes some complete flag of $V$, i.e.\ there exists a complete flag
   \[
    V = V_n \supsetneq V_{n-1} \supsetneq V_{n-2} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0 = 0,
   \]
   with $x(V_i) \subseteq V_i$ for every $x \in \g$ and $i = 0, \dotsc, n$.
  \item
   There exists a basis of $V$ with respect to which every $x \in \g$ is represented by an upper triangular matrix. In particular $\g$ is isomorphic to a Lie-subalgebra of $\tl_n(k)$ for $n = \dim V$.
 \end{enumerate}
\end{prop}


\begin{cor}
 A finite-dimensional $k$-Lie algebra $\g$ is solvable if and only if $[\g,\g]$ is nilpotent.
\end{cor}
\begin{proof}
 If $[\g,\g]$ is nilpotent then $[\g,\g]^{(i)} = 0$ for some $i \in \N$. Hence $\g^{(i+1)} = [\g,\g]^{(i)} = 0$, so $\g$ is solvable.
 
 Suppose that $\g$ is solvable. Then $\ad \g \cong \g/Z(\g)$ is a solvable subalgebra von $\gl(\g)$. By Lieâ€™s theorem there exists a basis of $\g$ with respect to which $\ad x$ is represented by an upper triangular matrix for each $x \in \g$. As $\ad$ is an homomorphism of Lie algebras it follows that with respect to this basis $\ad(x)$ is represented by a strictly upper triangular matrix for every $x \in [\g,\g]$. Hence every $x \in [\g,\g]$ is $\ad$-nilpotent, and therefore also $\ad_{[\g,\g]}$-nilpotent. By Engelâ€™s theorem $[\g,\g]$ is nilpotent.
\end{proof}


\begin{cor}\label{cor: irreducible representations of solvable Lie algebras are onedimenisonal}
 Every irreducible representation of a solvable Lie algebra $\g$ over $k$ is onedimensional.
\end{cor}
\begin{proof}
 Let $\rho \colon \g \to \gl(V)$ be an irreducible representation of $\g$, and therefore in particular $V \neq 0$. Then $\im \rho \subseteq \gl(V)$ is also solvable and by Lieâ€™s theorem there exists a common eigenvector $v \in V$, $v \neq 0$ for $\im \rho$. Because $x.v = \rho(x)(v) \in kv$ for every $x \in \g$ it follows that the onedimensional linear subspace $kv \subseteq V$ is a nonzero subrepresentation of $V$. Because $V$ is irreducible it follows that $V = kv$.
\end{proof}


\begin{rem}
 Corollary \ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} is actually equivalent to Lieâ€™s theorem: If $\g \subseteq \gl(V)$ is a Lie subalgebra with then $V$ is a representation of $\g$ via $x.v = x(v)$ for every $x \in \g$ and $v \in V$. If $V \neq 0$ is finite dimensional then $V$ contains an irreducible subrepresentation $U$ of $\g$ (simply take some nonzero subrepresentation of minimal dimension.) If $\g$ additionally is solvable then by Corollary \ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} the irreducible subrepresentation $U$ is onedimensional, hence of the form $U = kv$ for some $v \in V$ with $v \neq 0$. From the definition of the action of $\g$ on $V$ it follows that $v$ is common eigenvector of $\g$.
 
 As a consequence of this Lieâ€™s theorem as formulated in Theorem \ref{thrm: Lieâ€™s theorem} was called ``Lieâ€™s theorem -- concrete form'' in the lecture while Corollary \ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} was stated as ``Lieâ€™s theorem -- abstract version''.
\end{rem}








\subsection{The Killing form and Cartanâ€™s criterion}



\subsubsection{The Killing form}




\begin{defi}
 Let $\g$ be a Lie algebra over an arbitrary field $k$. A bilinear form $\beta \colon \g \times \g \to k$ is called \emph{associative} if
 \[
  \beta(x,[y,z]) = \beta([x,y],z) \quad \text{for all $x,y,z \in \g$}.
 \]
\end{defi}


\begin{defi}
 Let $V$ be a vector space and $\beta \colon \g \times \g \to k$ a symmetric bilinear form. Then
 \[
  \rad \beta \coloneqq \{x \in V \mid \text{$\beta(x,y) = 0$ for every $y \in V$}\}
 \]
 is the \emph{radical} of $\beta$.
\end{defi}


\begin{lem}\label{lem: orthogonal complement of an ideal is again an ideal}
 Let $\g$ be a Lie algebra over an arbitrary field $k$ and \mbox{$\beta \colon \g \times \g \to k$} a symmetric and associative bilinear form. For any ideal $I \subideal \g$ the orthogonal complement
 \[
  I^\perp \coloneqq \{y \in \g \mid \text{$\beta(x,y) = 0$ for every $x \in I$}\}
 \]
 is also an ideal in $\g$. In particular $\rad \beta = \g^\perp$ is an Ideal in $\g$.
\end{lem}
\begin{proof}
 For $z \in g$ and $y \in I^\perp$ it follows for every $x \in I$ that $[x,y] = 0$ and thus
 \[
  \beta(x,[y,z]) = \beta([x,y],z) = 0.
  \qedhere
 \]
\end{proof}


\begin{rem}
 The proof of Lemma \ref{lem: orthogonal complement of an ideal is again an ideal} did not use that $\beta$ is symmetric. This artficial restraint is only there to simplify the situation and notation (we do not need to distinguish between orthogonal complements from the left and from the right.) The main example of an associative bilinear form will be the Killing form, which is symmetric, so this assumption will pose no problems to us.
\end{rem}




\begin{defi}
 Let $\g$ be a finite dimensional Lie algebra over an arbitrary field $k$. The \emph{Killing form} of $\g$ is the bilinear form
 \[
  \kappa \colon \g \times \g \to k
  \quad\text{with}\quad
  \kappa(x,y) = \tr(\ad(x) \ad(y))
  \quad\text{for all $x,y \in \g$}.
 \]
\end{defi}


\begin{lem}\label{lem: killing form is associative and symmetric}
 The Killing form $\kappa$ of an finite dimensional Lie algebra $\g$ over an arbitrary field $k$ is associative and symmetric.
\end{lem}
\begin{proof}
 Recall from linear algebra that for any finite dimensional $k$-vector space $V$ and all endomorphisms $f_1, \dotsc, f_n \in \End_k(V)$
 \[
  \tr(f_1 \dotsm f_n) = \tr(f_2 \dotsm f_n f_1).
 \]
 For all $x,y \in \g$ it follows that
 \[
  \kappa(x,y) = \tr(\ad(x) \ad(y)) = \tr(\ad(y) \ad(x)) = \kappa(y,x),
 \]
 so $\kappa$ is symmetric. For all $x,y,z$ it follows that
 \begin{align*}
  \kappa(x,[y,z])
  &= \tr(\ad(x) \ad([y,z]))
  = \tr(\ad(x) [\ad(y), \ad(z)]) \\
  &= \tr(\ad(x) (\ad(y) \ad(z) - \ad(z) \ad(y))) \\
  &= \tr(\ad(x) \ad(y) \ad(z)) - \tr(\ad(x) \ad(z) \ad(y)) \\
  &= \tr(\ad(x) \ad(y) \ad(z)) - \tr(\ad(y) \ad(x) \ad(z)) \\
  &= \tr((\ad(x) \ad(y)- \ad(y) \ad(x)) \ad(z)) \\
  &= \tr([\ad(x), \ad(y)] \ad(z))
  = \tr(\ad([x,y]) \ad(z))
  = \kappa([x,y],z).
 \qedhere
 \end{align*}
\end{proof}


\begin{rem}
 If $\g$ is a Lie algebra and $\rho \colon \g \to \gl(V)$ a finite dimensional representation of $\g$ then the corresponding trace form $\phi_\rho$ is defined as
 \[
  \phi_\rho(x,y) \coloneqq \tr(\rho(x) \rho(y)) \quad \text{for all $x,y \in \g$}.
 \]
 Replacing $\ad$ with $\rho$ in the proof of Lemma \ref{lem: killing form is associative and symmetric} shows that $\phi_\rho$ is an associative and symmetric bilinear form on $\g$. The Killing form $\kappa$ of $\g$ is then just the special case $\kappa = \phi_{\ad}$.
\end{rem}




\subsubsection{The concrete Jordan decomposition}


\begin{defi}
 Let $V$ be an $n$-dimensional $k$-vector space and $x \in \End_k(V)$ (resp.\ $y \in \Mat_n(y)$). Then $x$ (resp.\ $y$) is called \emph{semisimple} if it is diagonalizable.
\end{defi}


\begin{rem}
 An endomorphism $x \in \End_k(V)$ as above is semisimple if and only if every $x$-invariant subspace of $V$ has a direct summand which is also $x$-invariant. (This depends on $k$ being algebraically closed.)
\end{rem}


\begin{thrm}\label{thrm: concrete Jordan decomposition}
 Let $V$ be a finite dimensional $k$-vector space and $x \in \End_k(V)$.
 \begin{enumerate}[leftmargin=*]
  \item
   There exist unique $x_s, x_n \in \End_k(V)$ satisfying the following properties:
   \begin{enumerate}
    \item
     $x = x_s + x_n$.
    \item
     $x_s$ is semisimple and $x_n$ is nilpotent.
    \item
     $x_s$ and $x_n$ commute.
   \end{enumerate}
  \item
   $x_s$ and $x_n$ are Polynomials in $x$ without constant term, i.e.\ there exist polynomials $P,Q \in k[T]$ such that $P(0) = Q(0) = 0$ and $x_s = P(x)$ and $x_n = P(x)$. In particular an endomorphism of $V$ commutes with $x$ if and only if it commutes with $x_s$ and $x_n$.
  \item
   If $A \subseteq B \subseteq V$ are linear subspaces with $x(B) \subseteq A$ then also $x_s(B) \subseteq A$ and $x_n(B) \subseteq A$.
 \end{enumerate}
\end{thrm}
\begin{proof}
 Let $\chi(T)$ be the characteristic polynomial of $x$ with $\chi(T) = \prod_{i=1}^n (T-\lambda_i)^{m_i}$ where $\lambda_i \neq \lambda_j$ for $i \neq j$. By the chinese reminder theorem the map
 \begin{equation}\label{eqn: use of chinese reminder theorem}
  \begin{aligned}
   k[T]/(\chi) &\longrightarrow \prod_{i=1}^n k[T]/((T-\lambda_i)^{m_i}),\\
   F + (\chi) &\longmapsto (F+((T-\lambda_1)^{m_1}), \dotsc, F+(((T-\lambda_n)^{m_n}))
  \end{aligned}
 \end{equation}
 is surjective. Thus there exists some polynomial $P \in k[T]$ with
 \begin{equation}\label{eqn: congruences of P}
  P(T) \equiv \lambda_i \mod (T-\lambda_i)^{m_i} \quad \text{for ever $i = 1, \dotsc, n$}.
 \end{equation}
 
 We can also assume that $P(0) = 0$. If $\lambda_i = 0$ for some $i$ then this follows directly from \eqref{eqn: congruences of P}. Otherwise the polynomials $(T-\lambda_1)^{m_1}$, \dots, $(T-\lambda_n)^{m_n}$, $T$ are pairwise coprime, so by replacing $\chi(T)$ with $\tilde{\chi}(T) \coloneqq \chi(T) T$ in \eqref{eqn: use of chinese reminder theorem} results in a polynomial $\tilde{P}$ which does not only satisfy \eqref{eqn: congruences of P} (with $P$ replaced by $\tilde{P}$) but also $\tilde{P} \bmod T = 0$.
 
 Now let $Q(T) \coloneqq T - P(T)$ as well as $x_s \coloneqq P(x)$ and $x_n \coloneqq Q(x)$. Then $x = x_s + x_n$ and $x_s$ and $x_n$ commute, as both are polynomials in $x$. For every $i = 1, \dotsc, n$ let
 \[
  V_i \coloneqq \ker (x-\lambda_i)^{m_i}
 \]
 be the generalized eigenspace of $x$ with respect to the eigenvalue $\lambda_i$. Is is known from linear algebra that $V = \bigoplus_{i=1}^n V_i$.
 
 It follows from \eqref{eqn: use of chinese reminder theorem} that for every $i = 1, \dotsc, n$ there exists some polynomial \mbox{$P_i \in k[T]$} with
 \[
  P(T) = \lambda_i + P_i(T) (T-\lambda_i)^{m_i},
 \]
 from which follows for every $v \in V_i$ that
 \[
  x_s(v)
  = (\lambda_i \id_V + P_i(x)(x-\lambda_i)^{m_i})(v)
  = \lambda_i v + P_i(x)(\underbrace{(x-\lambda_i)^{m_i}(v)}_{=0})
  = \lambda_i v.
 \]
 Hence $V_i$ is $x_s$-invariant with $x_s|_{V_i} = \lambda_i \id_{V_i}$ for every $i = 1, \dotsc, n$. As $V = \bigoplus_{i=1}^n V_i$ this shows that $x_s$ is semisimple and $V_i$ is precisely the eigenspace of $x_s$ to the eigenvalue $\lambda_i$.
 
 To see that $x_s$ is nilpotent notice that for every $i = 1, \dotsc, n$ and $v \in V_i$
 \[
  x_n(v) = x(v) - x_s(v) = x(v) - \lambda_i (v) = (x - \lambda_i \id_V)(v).
 \]
 Hence $V_i$ is $x_n$-invariant with $x_n|_{V_i} = x - \lambda_i \id_{V_i}$ for every $i = 1, \dotsc, n$. By definition of $V_i$ it follows that $x_n|_{V_i}$ is nilpotent for every $i = 1, \dotsc, n$. Because $V = \bigoplus_{i=1}^n V_i$ it follows that $x_n$ is nilpotent.
 
 This shows the existence of the claimed decomposition. For the uniqueness let $y_s, y_n \in \End_k(V)$ be any two endomorphisms with $x = y_s + y_n$ where $y_s$ is semisimple, $y_n$ is nilpotent and $y_s$ and $y_n$ commute. As $y_s$ and $y_n$ commute it follows that each of them commutes with $x = y_s + y_n$. Because $x_s$ and $x_n$ are polynomials in $x$ it follows from this that $y_s$ and $y_n$ both commute with $x_s$ and $x_n$. Hence $x_s$, $x_n$, $y_s$ and $y_n$ are pairwise commuting. In particular $x_s$ and $y_s$ are simultaneously diagonalizable which is why $x_s - y_s$ is also semisimple. As $x_n$ and $y_n$ commute and are both nilpotent it also follows that $y_n - x_n$ is nilpotent. But from $x_s + x_n = x = y_s + y_n$ it follows that
 \[
  \underbrace{x_s - y_s}_{\text{semisimple}} = \underbrace{y_n - x_n}_{\text{nilpotent}}.
 \]
 Hence $x_s - y_s = y_n - x_n = 0$.
 
 All other statements of the theorem directly follow from the construction of $x_s$ and $x_n$ and the fact that they are polynomials without constant term in $x$.
\end{proof}



\begin{rem}\label{rem: concrete Jordan decomposition for matrices}
 An analogeous statement of Theorem \ref{thrm: concrete Jordan decomposition} can be shown for $\Mat_n(k)$ instead of $\End_k(V)$. More precisely: Every matrix $x \in \Mat_n(k)$ can be uniquely decomposed into $x = x_s + x_n$ such that $x_s$ is semisimple, $x_n$ is nilpotent and $x_s$ and $x_n$ commute. Both $x_s$ and $x_n$ are polynomials without constant term in $x$, so any matrix in $\Mat_n(k)$ commutes with $x$ if and only if it commutes with $x_s$ and $x_n$. If $A \subseteq B \subseteq k^n$ are linear subspaces such that $B$ is carried into $A$ by left multiplication with $x$, then the same goes for $x_s$ and $x_n$.
\end{rem}


\begin{defi}
 Let $V$ be an $n$-dimensional vector space and $x \in \End_k(V)$ (resp.\ $y \in \Mat_n(k)$). Then the decomposition $x = x_s + x_n$ from Theorem \ref{thrm: concrete Jordan decomposition} (resp.\ the decomposition $y = y_s + y_n$ from Remark \ref{rem: concrete Jordan decomposition for matrices}) is called the \emph{concrete Jordandecomposition} of $x$ (resp.\ $y$). The element $x_s$ (resp.\ $y_s$) is called the \emph{semisimple part} of $x$ (resp.\ $y$) and the element $x_n$ (resp.\ $y_n$) is called the \emph{nilpotent part} of $x$ (resp.\ $y$).
\end{defi}











\subsubsection{Cartanâ€™s Criterion}


\begin{lem}
 Let $V$ be a finite dimensional $k$-vector space. Let $A \subseteq B \subseteq \gl(V)$ be linear subspaces and let
 \[
  T \coloneqq \{z \in \gl(V) \mid \ad(z)(B) \subseteq A\}.
 \]
 If $x \in T$ and $\tr(xz) = 0$ for every $z \in T$ then $x$ is nilpotent.
\end{lem}


\begin{lem}[Cartanâ€™s criterion for $\gl(V)$]
 Let $V$ be a finite dimensional $k$-vector space and $\g \subseteq \gl(V)$ a Lie subalgebra. Then $\g$ is solvable if and only if $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$.
\end{lem}


\begin{thrm}[Cartanâ€™s criterion for solvability]
 Let $\g$ be a finite dimensional $k$-Lie algebra. Then $\g$ is solvable if and only if
 \[
  \kappa(x,y) = 0 \quad \text{for every $x \in \g$ and $y \in [\g,\g]$}.
 \]
\end{thrm}













