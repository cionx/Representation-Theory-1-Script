\section{Nilpotent and solvable Lie algebras}





\subsection{Definition, examples and properties}


\begin{defi}
 Let $A$ be an associative $k$-algebra. An element $a \in A$ is called \emph{nilpotent} if $a^n = 0$ for some $n \geq 1$. Given a Lie algebra $\g$ an element $x \in \g$ is called \emph{$\ad$-nilpotent} if $\ad(x) \in \End_k(\g)$ is nilpotent.
\end{defi}


\begin{lem}\label{lem: nilpotent implies ad-nilpotent}
 If $A$ is an associative $k$-algebra and $x \in A$ nilpotent then $x$ is also $\ad$-nilpotent.
\end{lem}
\begin{proof}
 Let $\lambda_x \colon A \to A, a \mapsto xa$ and $\rho_x \colon A \to A, a \mapsto ax$. Because $x$ is nilpotent both $\lambda_x$ and $\rho_x$ are nilpotent. Because $A$ is associative $\lambda_x$ and $\rho_x$ commute. Hence $\ad(x) = \lambda_x - \rho_x$ is the sum of two commuting, nilpotent endomorphisms, and therefore also nilpotent.
\end{proof}


\begin{defi}
 Let $g$ be a Lie algebra. Define $\g^0 \coloneqq \g$ and $\g^{i+1} \coloneqq [\g,\g^i]$ for all $i \in \N$. Then
 \[
  \g = \g^0 \supseteq \g^1 \supseteq \g^2 \supseteq \dots
 \]
 is called the \emph{central series} of $\g$. Also define $\g^{(0)} \coloneqq \g$ and $\g^{(i+1)} \coloneqq [\g^{(i)},\g^{(i)}]$ for all $i \in \N$. Then
 \[
  \g^{(0)} \supseteq \g^{(1)} \supseteq \g^{(2)} \supseteq \dots
 \]
 is called the \emph{derived series} of $\g$. $\g$ is called \emph{nilpotent} if $\g^i = 0$ for some $i$ and \emph{solvable} if $g^{(i)} = 0$ for some $i$.
\end{defi}


\begin{expls}
 \begin{enumerate}[leftmargin=*]
  \item
   Every nilpotent Lie algebra $\g$ is also solvable because $\g^{(i)} \subseteq \g^i$ for every $i \in \N$.
  \item
   The upper triangular matrices $\tl_n(k)$ are solvable. But they are not nilpotent.
  \item
   The strictly upper triangular matrices $\nl_n(k)$ not only solvable but also nilpotent.
  \item
   If $n \geq 2$ then $\sll_2(\Cbb)$ is simple and therefore $[\sll_n(\Cbb),\sll_n(\Cbb)] = \sll_n(\Cbb)$. Since $[\gl_n(\Cbb),\gl_n(\Cbb)] = \sll_n(\Cbb)$ it follows that $\gl_n(\Cbb)$ is not solvable.
  \item
   If $\g$ is abelian then $\g$ is nilpotent and therefore also solvable.
  \item
   Every one-dimensional Lie algebra is abelian and therefore nilpotent and also solvable. The same goes for the two-dimensional abelian Lie algebra. The two-di\-men\-si\-o\-nal non-abelian Lie algebra $\g$ has a basis $x$,$y$ with $[x,y] = x$. Therefore $\g$ is solvable but not nilpotent.
  \item
   A \emph{Heisenberg Lie algebra} consists of a real vector space with basis $P_1, \dotsc, P_n$, $Q_1, \dotsc, Q_n$, $C$ together with the Lie bracket satisfying the following conditions:
   \[
    [P_i, P_j] = [Q_i, Q_j] = [P_i, C] = [Q_i, C] = 0
    \quad \text{and} \quad
    [P_i, Q_j] = \delta_{ij} C.
   \]
   This defines a nilpotent Lie algebra.
 \end{enumerate}
\end{expls}


\begin{prop}\label{prop: properties of solvable and nilpotent}
 Let $\g$ be a Lie algebra.
 \begin{enumerate}[leftmargin=*]
  \item
   If $\h$ is a Lie algebra and $f \colon \g \to \h$ a Lie algebras homomorphism then $f(\g)^i = f(\g^i)$ and $f(\g)^{(i)} = f(\g^{(i)})$ for all $i \geq 0$.
  \item
   If $\g$ is nilpotent (resp.\ solvable) then any Lie subalgebra $\h \subseteq \g$ and any quotient of $\g$ (by an ideal $I$) is nilpotent (resp.\ nilpotent).
  \item
   If $I \subideal \g$ with $I \subseteq Z(\g)$ and $\g/I$ is nilpotent then $\g$ is nilpotent.
  \item
   If $\g \neq 0$ is nilpotent then $Z(\g) \neq 0$.
  \item
   If $\g$ is nilpotent and $x \in \g$ then $x$ is $\ad$-nilpotent.
  \item
   If $I \subideal \g$ then $I^i$ and $I^{(i)}$ are ideals inside $\g$ for all $i \geq 0$.
 \end{enumerate}
\end{prop}
\begin{proof}
 \begin{enumerate}[leftmargin=*]
  \item
   It suficies to show that for any two subsets $X, Y \subseteq \g$
   \[
    f([X,Y])= [f(X),f(Y)]
   \]
   the statement then follows inductively. It holds because $f$ is a Lie algebra homomorphism and therefore
   \begin{align*}
    f([X,Y])
    &= f(\vspan_k \{[x, y] \mid x \in X, y \in Y\}) \\
    &= \vspan_k \{f([x, y]) \mid x \in X, y \in Y\} \\
    &= \vspan_k \{[f(x),f(y)] \mid x \in X, y \in Y\} \\
    &= \vspan_k \{[x',y'] \mid x' \in f(X), y' \in f(Y)\} \\
    &= [f(X),f(Y)].
   \end{align*}
  \item
   The statement about subalgebras follows from $\h^i \subseteq \g^i$ and $\h^{(i)} \subseteq \g^{(i)}$ for all $i \in \N$. The statement about quotient follow by using the canonical projection $\pi \colon \g \to \g/I$. Because $\pi$ is a Lie algebra homomorphism it follows that
   \[
    (\g/I)^i = \pi(\g)^i = \pi(\g^i) = 0
   \]
   for $i$ big enough. For solvable $\g$ the corresponding statements follow in the same way.
  \item
   Let $\pi \colon \g \to \g/I$ be the canonical projection. Because $\g/I$ is nilpotent there exists some $i \geq 0$ with $(\g/I)^i = 0$ and therefore
   \[
    0 = (\g/I)^i = \pi(\g)^i = \pi(\g^i).
   \]
   Thus $\g^i \subseteq I \subseteq Z(G)$ and hence $\g^{i+1} = 0$.
  \item
   Let $i \in \N$ be minimal with $\g^i \neq 0$ but $\g^{i+1} = 0$. Then $\g^i \subseteq Z(\g)$ and thus $Z(\g) \neq 0$.
  \item
   Since $\g$ is nilpotent there exists some $i \in \N$ with $\g^i = 0$. Then
   \[
    (\ad(x))^i(\g) \subseteq \g^i = 0,
   \]
   so $(\ad(x))^i = 0$.
  \item
   This follows inductively by using that $[I,J]$ is an ideal inside $\g$ for any $I,J \subideal \g$.
 \end{enumerate}
\end{proof}


\begin{cor}\label{cor: solvable via ses}
 If $I \subideal \g$ is an ideal inside a Lie algebra $\g$ then $\g$ is solvable if and only if both $I$ and $\g/I$ are solvable.
\end{cor}
\begin{proof}
 If $\g$ is solvable then $I$ and $\g/I$ are also solvable by Proposition~\ref{prop: properties of solvable and nilpotent}. Suppose that on the other hand both $I$ and $\g/I$ are solvable. Then there exists $i_1, i_2 \in \N$ with $(\g/I)^{(i_1)} = 0$ and $I^{i_2} = 0$. Let $\pi \colon \g \to \g/I, x \mapsto x + I$ be the canical projection. Because
 \[
  0 = (\g/I)^{(i_1)} = \pi(\g)^{(i_1)} = \pi(\g^{(i_1)})
 \]
 it follows that $\g^{(i_1}) \subseteq \ker \pi = I$. Thus
 \[
  \g^{(i_1 + i_2)} = (\g^{(i_1)})^{i_2} \subseteq I^{i_2} = 0,
 \]
 which shows that $\g$ is solvable.
\end{proof}


\begin{rem}
 The analogeous statement about nilpotency does not necessarily hold. Take for example the two-dimensional non-abelian Lie algebra $\g$, which has a basis $x,y$ with $[x,y] = x$. Then the one-dimensional linear subspace $I \coloneqq kx$ is an abelian ideal in $\g$ and in particular nilpotent. The quotient $\g/I$ is one-dimensional and therefore also nilpotent. But $\g$ itself is not nilpotent.
\end{rem}


\begin{cor}\label{cor: sum of solvable ideals is solvable}
 Let $\g$ be a Lie algebra and $I, J \subideal \g$ two solvable ideal. Then $I + J$ is also solvable.
\end{cor}
\begin{proof}
 Because $J$ is solvable the same goes for $J/(I \cap J)$. Hence in the short exact sequence
 \[
  0 \to I \to I+J \to (I+J)/I \to 0
 \]
 both $I$ and $(I+J)/I \cong J/(I \cap J)$ are solvable. Hence $I+J$ is solvable by Corollary~\ref{cor: solvable via ses}.
\end{proof}


\begin{defi}
 Let $\g$ be a finite dimenisonal Lie algebra. It follows from Corollary~\ref{cor: sum of solvable ideals is solvable} that $\g$ contains a unique maximal solvable ideal. This ideal is called the \emph{radical} of $\g$ and is denoted by $\rad \g$.
\end{defi}






\subsection{Engelâ€™s theorem}


From now on \emph{all} fields over which we work will be assumed to be algebraically closed, unless otherwise specified.


If $V$ is an $n$-dimensional vector space over $k$ and $x \in \End_k(V)$ a nilpotent endomorphism then $0$ is the only eigenvalue of $x$ (and occurs with multiplicity $n$) (here it is used that $k$ is algebraically closed). Hence there exists an eigenvector $v \in V$, $v \neq 0$ with $x(v) = 0$. The following proposition generalizes this observations for linear Lie algebras consisting of nilpotent endomorphisms.


\begin{prop}\label{prop: common eigenvector for nilpotent Lie algebras}
 Let $V \neq 0$ be a finite dimensional vector space and $\g \subseteq \gl(V)$ a Lie subalgebra such that every $x \in \g$ in nilpotent. Then there exists $v \in V$ with $v \neq 0$ and $x(v) = 0$ for every $x \in \g$, i.e.\ $v$ is a common eigenvector of all $x \in \g$ (all of which are nilpotent and thus have $0$ as their only eigenvalue).
\end{prop}
\begin{proof}
 The statement can be then shown by induction over $\dim \g$. For $\dim \g = 0$ the statement follows from $\g = 0$ and for $\dim \g = 1$ the statement follows as previously discussed from $\g = k x$ with $x$ being a nilpotent endomorphism of $V$.
 
 So let $\dim \g \geq 2$ and suppose that the statement holds for all smaller dimensions. Let $I \subseteq \g$ be a maximal proper Lie subalgebra (such a subalgebra exists because it is precisely one of maximal dimension strictly smaller than $n$). Any $x \in \g$ with $x \neq 0$ spans a one-dimensional subalgebra $k x$ of $\g$; because $\dim \g \geq 2$ it is a proper one. This show that $I \neq 0$. It turns out that $I$ is already in ideal in $\g$:
 
 By assumption $\g$ consists of nilpotent endomorphisms and therefore of $\ad$-nilpotent elemenents. In particular every $x \in I$ acts nilpotent on $\g$ via $\ad(x)$ with $I$ being an $\ad(x)$-invariant linear subspace. Therefore every $x \in I$ acts on the quotient vector space $\g/I$ by an induced nilpotent endomorphism
 \[
  \overline{\ad}(x) \colon \g/I \to \g/I, \quad y + I \mapsto \ad(x)(y) + I = [x,y] + I.
 \]
 As the map $\overline{\ad} \colon I \to \gl(\g/I)$ is an homomorphism of Lie algebras (because $\ad$ is) the image $\{\overline{\ad}(x) \mid x \in I\} \subseteq \gl(\g/I)$ is an Lie subalgebra, consisting of nilpotent endomorphisms. From $I \neq 0$ it follows that $\dim \g/I < \dim \g$, so by induction assumption there exists some $y \in \g$ with $\overline{\ad}(x)(y+I) = 0$ for every $x \in I$ and $y+I \neq 0+I$. Hence $[x,y] \in I$ for every $x \in I$ but $y \neq I$. Hence $y \in N_\g(I)$ with $y \neq I$, so $I$ is properly contained in its normalizer. As $I$ is a maximal proper subalgebra of $\g$ it follows that $N_\g(I) = \g$, so $I$ is an ideal. It is even one of codimension $1$:
 
 If $I$ had not codimension $1$ then $\dim \g/I > 1$. Then $\g/I$ contains a one-dimensional proper subalgebra $L$ (as seen above), and the preimage $\pi^{-1}(J)$ under the canonical projection $\pi \colon \g \to \g/I$ is then a proper subalgebra of $\g$ properly containing $I$, which contradicts the maximality of $I$. Hence $I$ has codimension $1$.
 
 As $I \subseteq \g$ has codimension $1$ there exists some $y \in \g$ with $g = I \oplus ky$ (as vector spaces). Because $\dim I < \dim \g$ it follows from the induction assumption that
 \[
  U \coloneqq \{v \in V \mid \text{$x(v) = 0$ for every $x \in I$}\} = \bigcap_{x \in I} \ker x
 \]
 is a nonzero linear subspace of $V$. It sufficies to show that $U$ is $y$-invariant: Then there exists some eigenvector $u \in U$ of $y$ for which necessarily $y(u) = 0$. If $u\in U$ then $[x,y] \in I$ for every $x \in I$ because $I \subideal \g$ and therefore
 \[
  x(y(u)) = [x,y](u) - y(x(u)) = 0 - y(0) = 0.
 \]
 Hence $y(u) \in U$, so $U$ is $y$-invariant.
\end{proof}


\begin{prop}\label{prop: stuff for Engels theorem}
 Let $V$ be a finite dimensional vector space with $n = \dim V$ and $\g \subseteq \gl(V)$ a Lie subalgebra. Then the following are equivalent:
 \begin{enumerate}
  \item\label{enum: engels g consists of nilpotent endomorphisms}
   $\g$ consists of nilpotent endomorphisms.
  \item\label{enum: engels there exists a complete flag shifted by g}
   There exists a complete flag of $V$
   \[
    V = V_n \supsetneq V_{n-1} \supsetneq V_{n-2} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0 = 0,
   \]
   with $x(V_i) \subseteq V_{i-1}$ for every $i = 1, \dotsc, n$.
  \item\label{enum: engels represented by strictly upper triangular matrices}
   There exists a basis of $V$ with respect to which every $x \in \g$ is represented by an strictly upper triangular matrix.
 \end{enumerate}
\end{prop}
\begin{proof}
 The implication \ref{enum: engels g consists of nilpotent endomorphisms} $\Rightarrow$ \ref{enum: engels there exists a complete flag shifted by g} can be shown by induction over $\dim V$. For $\dim V = 1$ set $V_0 \coloneqq 0$ and $V_1 \coloneqq V$. By assumption every $x \in \g$ acts nilpotent on $V$, so $x(V) = 0$ because $V$ is one-dimensional. Thus $V = V_1 \supsetneq V_0 = 0$ is a complete flag for $V$ satisfying the condititons.
   
 Now let $\dim V = n \geq 2$ and suppose the statement holds for all smaller dimensions. Let $v \in V$, $v \neq 0$ with $x(v) = 0$ for every $x \in \g$ and $W \coloneqq V / kv$. Every $x \in \g$ induces an endomorphism
 \[
  \overline{x} \colon W \to W, \quad v + kv \mapsto x(v) + kv.
 \]
 By induction assumption exists a complete flag
 \[
  W = W_{n-1} \supsetneq W_{n-2} \supsetneq W_{n-3} \supsetneq \dotsb \supsetneq W_1 \supsetneq W_0 = 0
 \]
 with $\overline{x}(W_i) \subseteq W_{i-1}$ for every $x \in \g$ and $i = 1, \dotsc, n-1$. By setting $V_i \coloneqq \pi^{-1}(W_{i-1})$ for every $i = 1, \dotsc, n$ and $V_0 = 0$ it follows that
 \[
  V = V_n \supsetneq V_{n-1} \supsetneq V_{n-2} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0 = 0,
 \]
 is a complete flag of $V$. On the one hand $x(V_1) = x(kv) = 0 = V_0$ for every $x \in \g$ and on the other hand
 \[
  \pi(x(V_i))
  = \overline{x}(\pi(V_i))
  = \overline{x}(W_{i-1})
  \subseteq W_{i-2}
 \]
 and therefore $x(V_i) \subseteq \pi^{-1}(W_{i-2}) = V_{i-1}$ for every $i = 2, \dotsc, n$.
 
 The implications \ref{enum: engels there exists a complete flag shifted by g} $\Rightarrow$ \ref{enum: engels represented by strictly upper triangular matrices} and \ref{enum: engels represented by strictly upper triangular matrices} $\Rightarrow$ \ref{enum: engels g consists of nilpotent endomorphisms} are basic facts from linear algebra.
\end{proof}


\begin{thrm}[Engel]
 Let $\g$ be a finite dimensional Lie algebra. Then $\g$ is nilpotent if and only if all its elements are $\ad$-nilpotent.
\end{thrm}
\begin{proof}
 If $\g$ is nilpotent then there exists some $i \in \N$ with $\g^i = 0$, from which is follows from $\ad(x)^i(y) \in \g^i$ for every $x,y \in \g$ that $\ad(x)^i = 0$ for every $x \in \g$, hence every $x \in \g$ is $\ad$-nilpotent.
 
 On the other hand suppose that $\g$ consists of $\ad$-nilpotent elemenents. If $\g = Z(\g)$ then $\g$ is abelian and hence nilpotent, so it sufficies to show the statement under the additional assumption that $Z(\g) \subsetneq \g$. Because $\g/Z(\g) \cong \ad \g$ is a Lie subalgebra of $\gl(\g)$ consisting of nilpotent Elemenents it follows from Proposition~\ref{prop: stuff for Engels theorem} that $\g/Z(\g)$ is isomorphic to a Lie subalgebra of $\nl_n(k)$ for $n = \dim \g/Z(\g) \geq 1$. Because $\nl_n(k)$ is nilpotent the same goes for $\g/Z(\g)$ as seen in Proposition~\ref{prop: properties of solvable and nilpotent}.
\end{proof}


\begin{rem}
 It is not true that every nilpotent Lie-subalgebra $\g \subseteq \gl(V)$ with $V$ being a finite dimensional vector space is represented by upper triangular matrices with respect to some basis of $V$. For example the onedimensional subalgebra $k \id_V \subseteq \gl(V)$ is abelian and hence nilpotent, but with respect to every basis of $V$ represented by $kI \subseteq \gl_n(k)$.
\end{rem}


%TODO: Give a compromise using the abstract Jordan decomposition




\subsection{Lieâ€™s theorem}


From now on we will not only require \emph{every} field $k$ we work with to be algebraically closed, but also to be of characteristic $0$. Unless otherwise stated this holds up to the last page (page \pageref{LastPage}) of this text. In particular all Lie algebras and vector spaces will be assumed to have such a field as their ground field, even if not explicitely stated.


\begin{defi}
 Let $V$ be a representation of a Lie algebra $\g$. For $\lambda \in \g^*$ the linear subspace
 \[
  V_\lambda \coloneqq \{v \in V \mid \text{$x.v = \lambda x$ for every $x \in \g$}\}
 \]
 is called the \emph{$\g$-weight space} of $V$ with \emph{weight} $\lambda$.
\end{defi}



\begin{lem}[Invariance Lemma]
 Let $V$ be a finite dimensional representation of a Lie algebra $\g$ and $I \subideal \g$ an ideal. Then $V$ is also a representation of $I$ by restriction of the action of $\g$ on $V$ to $I$. For $\lambda \in I^*$ let $V_\lambda$ be the $I$-weight space of $V$ with weight $\lambda$. Then $V_\lambda$ is a subrepresentation of $\g$.
\end{lem}
\begin{proof}
 For $v \in V$ and $x_1, \dotsc, x_n \in \g$ we will write
 \[
  x_1 \dotsm x_n v \coloneqq x_1.(\dotsc.(x_n.v)).
 \]
 If $V_\lambda = 0$ the statement is clear, so for this proof we fix some $\lambda \in I^*$ with $V_\lambda \neq 0$.
 
 That $V_\lambda$ is a subrepresentation of $\g$ means that $yv \in V_\lambda$ for every $y \in \g$ and $v \in V_\lambda$, which is equivalent to $xyv = \lambda(x)yv$ for every $x \in I$, $y \in \g$ and $v \in V_\lambda$. Because
 \[
  xyv = [x,y]v + yxv = \lambda([x,y])v + \lambda(x)yv \quad \text{for every $x \in I$, $y \in \g$ and $v \in V_\lambda$}
 \]
 this is equivalent to $\lambda([x,y]) = 0$ for every $x \in I$ and $y \in \g$.
 
 Until further notice we fix some $y \in \g$ and $v \in V_\lambda$ with $v \neq 0$. As $V$ is finite dimensional there exists some maximal $n \geq 1$ such that $v$, $yv$, \dots, $y^n v$ are linearly independent. Let
 \[
  W_i \coloneqq \vspan_k(v, yv, \dotsc, y^i v) \quad \text{for every $i = 0, \dotsc, n$}.
 \]
 Because $v$, $yv$, \dots, $y^n v$, $y^{n+1} v$ are linearly dependent it follows that $W_n$ is invariant under the action of $y$.
 
 \begin{claim*}
  The linear subspace $W_i$ is for every $i = 0, \dotsc, n$ a subrepresentation of $I$. With respect to the basis $w$, $yw$, \dots, $y^i w$ of $W_i$ the action of $x \in I$ is represented by an upper triangular matrix where every diagonal entry is $\lambda(x)$.
 \end{claim*}
 \begin{proof}
  The claim can be proven by induction over $i$. As $W_0 = kv$ with $xv = \lambda(x)v$ for every $x \in I$ the claim holds for $i = 0$. Suppose that $i < n$ and that the claim holds for $W_0$, \dots, $W_i$. If $x \in I$ then also $[x,y] \in I$ and therefore
  \[
   x y^{i+1} v
   = \underbrace{[x,y] y^i v}_{\mathclap{\substack{\in W_i \\ \text{by induction}}}} + y x y^i v
   \equiv y x y^i v
   \mod W_i.
  \]
  By induction it is not only $x y^i v \in W_i$ but also $x y^i v + W_{i-1} = \lambda(x) y^i v + W_{i-1}$. Therefore
  \[
   y x y^i v \equiv \lambda(x) y^{i+1} v \mod W_i.
  \]
  This shows the claim for $W_{i+1}$.
 \end{proof}
 
 Let $x \in I$. As $[x,y] \in I$ it follows from the previous claim that the $(n+1)$-dimensional linear subspace $W_n$ is invariant under the action of $[x,y]$, which is given by an endomorphism $\phi_{[x,y]} \in \End_k(W_n)$, and that $\phi_{[x,y]}$ is represented by an upper triangular matrix for which all diagonal entries are $\lambda([x,y])$. It follows that in particular
 \begin{equation}\label{eqn: invariance lemma zero trace}
  \tr \phi_{[x,y]} = (n+1) \lambda([x,y])
 \end{equation}
 On the other hand $W_n$ is invariant under the action of both $x$ (by the claim) and $y$, which act by endomorphisms $\phi_x, \phi_y \in \End_k(V)$. As $V$ is a representation of the Lie algebra $\g$ it follows that $\phi_{[x,y]} = [\phi_x, \phi_y]$ and thus $\tr \phi_{[x,y]} = 0$. Together with \eqref{eqn: invariance lemma zero trace} it follows that $\lambda([x,y]) = 0$.
\end{proof}


As a generalization of Proposition~\ref{prop: common eigenvector for nilpotent Lie algebras} we have the following result about solvable linear Lie algebras.


\begin{thrm}[Lie]\label{thrm: Lieâ€™s theorem}
 Let $V \neq 0$ be a finite dimensional $k$-vector space and $\g \subseteq \gl(V)$ a solvable Lie subalgebra. Then there exists a common eigenvector for $\g$, i.e.\ some $v \in V$, $v \neq 0$ with $x(v) \in k v$ for every $x \in \g$.
\end{thrm}
\begin{proof}
 The statement can be shown by induction over $\dim \g$. If $\dim \g = 0$ then $\g = 0$ and any $v \in V$ with $v \neq 0$ does the job. If $\dim \g = 1$ then $\g = k x$ for some $x \in \gl(V)$ with $x \neq 0$. Then any eigenvector of $x$ does the job (since $k$ is assumed to be algebraically closed and $V \neq 0$ such an eigenvector does exist).
 
 Suppose that $\dim \g = n \geq 2$ and the statement holds for every smaller dimension. Similarly to the proof of Proposition~\ref{prop: common eigenvector for nilpotent Lie algebras} we will split this proof into four consecutive parts:
 \begin{enumerate}
  \item
   Finding an ideal $I \subideal \g$ of codimension $1$.
  \item
   Finding common eigenvectors for $I$ by induction.
  \item
   Showing that $\g$ stabilizes as nonzero subspace $U \subseteq V$ of such eigenvectors.
  \item
   Writing $\g = I \oplus k y$ (as vector spaces) and finding an eigenvector of $y$ in $U$.
 \end{enumerate}
 
 For the first step notice that $\g$ is nonzero but solvable, so $[\g,\g] \subideal \g$ is a proper ideal. Hence $\g/[\g,\g]$ is nonzero abelian Lie algebra. Therefore there exists a linear subspace $J \subseteq \g/[\g,\g]$ of codimension $1$ and $J$ is an ideal in $\g/[\g,\g]$. Hence the preimage $I = \pi^{-1}(J)$ for the canonical projection $\g \to \g/[\g,\g], x \mapsto x + [\g,\g]$ is an ideal in $\g$ of codimension $1$.
 
 For the second step notice that because $\g$ is solvable the same goes for $I$. So by induction hypothesis there exists a common eigenvector for $I$. Hence there exists some $\lambda \in I^*$ with $U \coloneqq V_\lambda \neq 0$, where we view $V$ as a representation of $\g$ via $x.v = x(v)$ for every $x \in \g$ and $v \in V$.
 
 The third step follows directly from the invariance lemma.
 
 For the fourth step let $y \in \g$ with $\g = I \oplus k y$ (as vector spaces). Since $\g$ stabilizes $U$ this holds in particular for $y$. As $U \neq 0$ it follows that there exists some eigenvector of $y$ inside of $U$, which is then a common eigenvector for $\g$.
\end{proof}


\begin{rem}
 The proof for Lieâ€™s theorem given in the lecture is basically a less structured version of the one in \cite[\S 4.1]{Humphreys}, from where we took the idea of breaking down the proof into four steps to emphasize the similarities with the proof of Proposition~\ref{prop: common eigenvector for nilpotent Lie algebras} (which we found very useful for understanding the structure of the previous proof).
\end{rem}


\begin{prop}\label{prop: common eigenvector for solvable Lie algebras}
 Let $V \neq 0$ be a finite dimensional $k$-vector space with $n = \dim V$ and $\g \subseteq \gl(V)$ a Lie subalgebra. Then the following are equivalent:
 \begin{enumerate}[leftmargin=*]
  \item
   $\g$ is solvable.
  \item
   $\g$ stabilizes some complete flag of $V$, i.e.\ there exists a complete flag
   \[
    V = V_n \supsetneq V_{n-1} \supsetneq V_{n-2} \supsetneq \dotsb \supsetneq V_1 \supsetneq V_0 = 0,
   \]
   with $x(V_i) \subseteq V_i$ for every $x \in \g$ and $i = 0, \dotsc, n$.
  \item
   There exists a basis of $V$ with respect to which every $x \in \g$ is represented by an upper triangular matrix. In particular $\g$ is isomorphic to a Lie-subalgebra of $\tl_n(k)$ for $n = \dim V$.
 \end{enumerate}
\end{prop}


\begin{cor}
 A finite-dimensional $k$-Lie algebra $\g$ is solvable if and only if $[\g,\g]$ is nilpotent.
\end{cor}
\begin{proof}
 If $[\g,\g]$ is nilpotent then $[\g,\g]^{(i)} = 0$ for some $i \in \N$. Hence $\g^{(i+1)} = [\g,\g]^{(i)} = 0$, so $\g$ is solvable.
 
 Suppose that $\g$ is solvable. Then $\ad \g \cong \g/Z(\g)$ is a solvable subalgebra von $\gl(\g)$. By Lieâ€™s theorem there exists a basis of $\g$ with respect to which $\ad x$ is represented by an upper triangular matrix for each $x \in \g$. As $\ad$ is an homomorphism of Lie algebras it follows that with respect to this basis $\ad(x)$ is represented by a strictly upper triangular matrix for every $x \in [\g,\g]$. Hence every $x \in [\g,\g]$ is $\ad$-nilpotent, and therefore also $\ad_{[\g,\g]}$-nilpotent. By Engelâ€™s theorem $[\g,\g]$ is nilpotent.
\end{proof}


\begin{cor}\label{cor: irreducible representations of solvable Lie algebras are onedimenisonal}
 Every irreducible representation of a solvable Lie algebra $\g$ over $k$ is onedimensional.
\end{cor}
\begin{proof}
 Let $\rho \colon \g \to \gl(V)$ be an irreducible representation of $\g$, and therefore in particular $V \neq 0$. Then $\im \rho \subseteq \gl(V)$ is also solvable and by Lieâ€™s theorem there exists a common eigenvector $v \in V$, $v \neq 0$ for $\im \rho$. Because $x.v = \rho(x)(v) \in kv$ for every $x \in \g$ it follows that the onedimensional linear subspace $kv \subseteq V$ is a nonzero subrepresentation of $V$. Because $V$ is irreducible it follows that $V = kv$.
\end{proof}


\begin{rem}
 Corollary~\ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} is actually equivalent to Lieâ€™s theorem: If $\g \subseteq \gl(V)$ is a Lie subalgebra with then $V$ is a representation of $\g$ via $x.v = x(v)$ for every $x \in \g$ and $v \in V$. If $V \neq 0$ is finite dimensional then $V$ contains an irreducible subrepresentation $U$ of $\g$ (simply take some nonzero subrepresentation of minimal dimension.) If $\g$ additionally is solvable then by Corollary~\ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} the irreducible subrepresentation $U$ is onedimensional, hence of the form $U = kv$ for some $v \in V$ with $v \neq 0$. From the definition of the action of $\g$ on $V$ it follows that $v$ is common eigenvector of $\g$.
 
 As a consequence of this Lieâ€™s theorem as formulated in Theorem~\ref{thrm: Lieâ€™s theorem} was called ``Lieâ€™s theorem -- concrete form'' in the lecture while Corollary~\ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} was stated as ``Lieâ€™s theorem -- abstract version''.
\end{rem}


\begin{rem}
 Corollary~\ref{cor: irreducible representations of solvable Lie algebras are onedimenisonal} does not hold for general fields $k$, even if algebraically closed. To see this let $k$ be an algebraically closed field with $\chara k = 2$ and $\g \coloneqq \sll_2(k)$. Then $\g$ has a basis $(e,h,f)$ where
 \begin{gather*}
  e = \vect{0 & 1 \\ 0 & 0}, \quad
  h = \vect{1 & 0 \\ 0 & -1} = \vect{1 & 0 \\ 0 & 1}, \quad
  f = \vect{0 & 0 \\ 1 & 0}
 \shortintertext{with}
  [h,e] = [h,f] = 0 \quad\text{and}\quad [e,f] = h.
 \end{gather*}
 Hence $\g$ is solvable. Let $V \coloneqq k^2$ be the natural representation of $\g$, i.e.\ $\g$ acts on $V$ by $x.v = x(v)$ for every $x \in \g$ and $v \in V$. Then
 \[
  e.\vect{x \\ y} =  \vect{y \\ 0}
  \quad \text{and} \quad
  f.\vect{x \\ y} = \vect{0 \\ x}
  \quad \text{for every $\vect{x \\ y} \in V$}.
 \]
 It follows that if $U \subseteq V$ is a nonzero subrepresentation then $U$ contains either $e_1$ or $e_2$, and therefore also the other one. Hence $U = V$, which shows that $V$ is an irreducible representation of $\g$. 
\end{rem}









\subsection{The Killing form and Cartanâ€™s criterion}



\subsubsection{The Killing form}




\begin{defi}
 Let $\g$ be a Lie algebra over an arbitrary field $k$. A bilinear form $\beta \colon \g \times \g \to k$ is called \emph{associative} if
 \[
  \beta(x,[y,z]) = \beta([x,y],z) \quad \text{for all $x,y,z \in \g$}.
 \]
\end{defi}


\begin{rem}
 As $\g$ acts on itself by the adjoint representation it also acts on $(\g \otimes_k \g)^*$ such that for every $\beta \in (\g \otimes_k \g)^*$ and all $x,y,z \in \g$
 \begin{align*}
  (y.\beta)(x \otimes z)
  &= -\beta(y.(x \otimes z))
  = -\beta((y.x) \otimes z + x \otimes (y.z)) \\
  &= -\beta([y,x] \otimes z) -\beta(x \otimes [y,z])
  = \beta([x,y] \otimes z) - \beta(x \otimes [y,z]).
 \end{align*}
 
 \begin{samepage} Identifying $(\g \otimes_k \g)^*$ with the bilinear forms on $\g$ via the universal property of the tensor product it follows that a bilinear form $\beta \colon \g \times \g \to k$ is associative if and only if
 \[
  y.\beta = 0 \quad \text{for every $y \in \g$}.
 \]
 Because of this associative bilinear forms on $\g$ are also called \emph{invariant}. \end{samepage}
\end{rem}



\begin{defi}
 Let $V$ be a vector space and $\beta \colon \g \times \g \to k$ a symmetric bilinear form. Then
 \[
  \rad \beta \coloneqq \{x \in V \mid \text{$\beta(x,y) = 0$ for every $y \in V$}\}
 \]
 is the \emph{radical} of $\beta$.
\end{defi}


\begin{lem}\label{lem: orthogonal complement of an ideal is again an ideal}
 Let $\g$ be a Lie algebra over an arbitrary field $k$ and \mbox{$\beta \colon \g \times \g \to k$} a symmetric and associative bilinear form. For any ideal $I \subideal \g$ the orthogonal complement
 \[
  I^\perp \coloneqq \{y \in \g \mid \text{$\beta(x,y) = 0$ for every $x \in I$}\}
 \]
 is also an ideal in $\g$. In particular $\rad \beta = \g^\perp$ is an Ideal in $\g$.
\end{lem}
\begin{proof}
 For $z \in g$ and $y \in I^\perp$ it follows for every $x \in I$ that $[x,y] = 0$ and thus
 \[
  \beta(x,[y,z]) = \beta([x,y],z) = 0.
  \qedhere
 \]
\end{proof}


\begin{rem}
 The proof of Lemma~\ref{lem: orthogonal complement of an ideal is again an ideal} did not use that $\beta$ is symmetric. This artficial restraint is only there to simplify the situation and notation (we do not need to distinguish between orthogonal complements from the left and from the right.) The main example of an associative bilinear form will be the Killing form, which is symmetric, so this assumption will pose no problems to us.
\end{rem}


\begin{defi}
 Let $\g$ be a finite dimensional Lie algebra over an arbitrary field $k$. The \emph{Killing form} of $\g$ is the bilinear form
 \[
  \kappa \colon \g \times \g \to k
  \quad\text{with}\quad
  \kappa(x,y) = \tr(\ad(x) \ad(y))
  \quad\text{for all $x,y \in \g$}.
 \]
\end{defi}


\begin{lem}\label{lem: killing form is associative and symmetric}
 The Killing form $\kappa$ of an finite dimensional Lie algebra $\g$ over an arbitrary field $k$ is associative and symmetric.
\end{lem}
\begin{proof}
 Recall from linear algebra that for any finite dimensional $k$-vector space $V$ and all endomorphisms $f_1, \dotsc, f_n \in \End_k(V)$
 \[
  \tr(f_1 \dotsm f_n) = \tr(f_2 \dotsm f_n f_1).
 \]
 For all $x,y \in \g$ it follows that
 \[
  \kappa(x,y) = \tr(\ad(x) \ad(y)) = \tr(\ad(y) \ad(x)) = \kappa(y,x),
 \]
 so $\kappa$ is symmetric. For all $x,y,z$ it follows that
 \begin{align*}
  \kappa(x,[y,z])
  &= \tr(\ad(x) \ad([y,z]))
  = \tr(\ad(x) [\ad(y), \ad(z)]) \\
  &= \tr(\ad(x) (\ad(y) \ad(z) - \ad(z) \ad(y))) \\
  &= \tr(\ad(x) \ad(y) \ad(z)) - \tr(\ad(x) \ad(z) \ad(y)) \\
  &= \tr(\ad(x) \ad(y) \ad(z)) - \tr(\ad(y) \ad(x) \ad(z)) \\
  &= \tr((\ad(x) \ad(y)- \ad(y) \ad(x)) \ad(z)) \\
  &= \tr([\ad(x), \ad(y)] \ad(z))
  = \tr(\ad([x,y]) \ad(z))
  = \kappa([x,y],z).
 \qedhere
 \end{align*}
\end{proof}


\begin{expl}\label{expl: Killing form of gl_n}
 Let $\g \coloneqq \gl_n(k)$ for some arbitrary field $k$. Then
 \[
  \kappa(x,y) = 2n\tr(xy) - 2(\tr x)(\tr y) \eqqcolon \beta(x,y).
  \quad \text{for all $x,y \in \g$}.
 \]
 To see this let $(e_{ij})_{i,j=1,\dotsc,n}$ be the standard basis of $\gl_n(k)$, i.e.\ the $(i,j)$-th entry of $e_{ij}$ is $1$, all other entries are $0$ (hence $e_{ij} e_k = \delta_{jk} e_i$ for every $j = 1, \dotsc, n$ where $(e_1, \dotsc, e_n)$ is the standard basis of $k^n$). In particular
 \begin{equation}\label{eqn: product of elementary matrices}
  e_{ij} e_{kl} = \delta_{jk} e_{il} \quad \text{for all $i,j,k,l = 1, \dotsc, n$}.
 \end{equation}

 To show that $\kappa = \beta$ is sufficies to show that
 \[
  \kappa(e_{ij}, e_{kl}) = \beta(e_{ij}, e_{kl})
  \quad \text{for all $i,j,k,l = 1, \dotsc, n$},
 \]
 as both $\kappa$ and $\beta$ are bilinear forms on $\g$. For all $k,l,g,h = 1, \dotsc, n$ it follows from \eqref{eqn: product of elementary matrices} that
 \[
  \ad(e_{kl})(e_{gh})
  = [e_{kl}, e_{gh}]
  = e_{kl} e_{gh} - e_{gh} e_{kl}
  = \delta_{lg} e_{kh} - \delta_{kh} e_{gl}.
 \]
 It follows that for all $i,j,k,l,g,h = 1, \dotsc, n$
 \begin{align*}
  \ad(e_{ij})\ad(e_{kl})(e_{gh})
  &= \ad(e_{ij})(\delta_{lg} e_{kh} - \delta_{kh} e_{gl})
  = \delta_{lg} \ad(e_{ij})(e_{kh}) - \delta_{kh} \ad(e_{ij})(e_{gl}) \\
  &= \delta_{lg}(\delta_{jk} e_{ih} - \delta_{ih} e_{kj}) - \delta_{kh}(\delta_{jg} e_{il} - \delta_{il} e_{gj}) \\
  &= \delta_{jk}\delta_{lg} e_{ih} - \delta_{ih} \delta_{lg} e_{kj} - \delta_{jg} \delta_{kh} e_{il} + \delta_{il} \delta_{kh} e_{gj}
 \end{align*}
 and the coefficient of $e_{gh}$ in this expression is
 \[
  a_{gh}
  = \delta_{jk} \delta_{lg} \delta_{ig}
    + \delta_{il} \delta_{kh} \delta_{jh}
    - \delta_{ih} \delta_{lg} \delta_{kg} \delta_{jh}
    - \delta_{jg} \delta_{kh} \delta_{ig} \delta_{hl}.
 \]
 It follows that for all $i,j,k,l = 1, \dotsc, n$
 \begin{align*}
  \kappa(e_{ij}, e_{kl})
  &= \sum_{g,h=1}^n ( \delta_{jk} \delta_{lg} \delta_{ig}
     + \delta_{il} \delta_{kh} \delta_{jh}
     - \delta_{ih} \delta_{lg} \delta_{kg} \delta_{jh}
     - \delta_{jg} \delta_{kh}\delta_{ig} \delta_{hl} ) \\
  &= \sum_{g,h=1}^n \delta_{jk} \delta_{lg} \delta_{ig}
     + \sum_{g,h=1}^n \delta_{il} \delta_{kh} \delta_{jh}
     - \sum_{g,h=1}^n \delta_{ih} \delta_{lg} \delta_{kg} \delta_{jh}
     - \sum_{g,h=1}^n\delta_{jg} \delta_{kh}\delta_{ig} \delta_{hl} \\
  &= n \delta_{jk} \delta_{il} + n \delta_{il} \delta_{jk} - \delta_{ij} \delta_{kl} - \delta_{ij} \delta_{kl}
  = 2n \delta_{il} \delta_{jk} - 2 \delta_{ij} \delta_{kl} \\
  &= 2n \delta_{jk} (\tr e_{il})  - 2 (\tr e_{ij})(\tr e_{kl})
  = 2n \tr(e_{ij} e_{kl}) - 2(\tr e_{ij})(\tr e_{kl}) \\
  &= \beta(e_{ij}, e_{kl}).
 \end{align*}
\end{expl}


\begin{rem}
 If $\g$ is a Lie algebra and $\rho \colon \g \to \gl(V)$ a finite dimensional representation of $\g$ then the corresponding trace form $\phi_\rho$ is defined as
 \[
  \phi_\rho(x,y) \coloneqq \tr(\rho(x) \rho(y)) \quad \text{for all $x,y \in \g$}.
 \]
 Replacing $\ad$ with $\rho$ in the proof of Lemma~\ref{lem: killing form is associative and symmetric} shows that $\phi_\rho$ is an associative and symmetric bilinear form on $\g$. The Killing form $\kappa$ of $\g$ is then just the special case $\kappa = \phi_{\ad}$.
\end{rem}


\begin{lem}\label{lem: restriction of the Killing form to an ideal}
 Let $\g$ be a finite-dimensional Lie algebra over an arbitrary field $k$. Then for any ideal $I \subseteq \g$ the Killing form $\kappa_I$ is given by restriction of the Killing form $\kappa_\g$ to $I$, i.e.\ $\kappa_I = \kappa_\g|_{I \times I}$.
\end{lem}
\begin{proof}
 Let $x, y \in I$. Then $I$ is $\ad_\g(x)$-invariant. Let $(x_1, \dotsc, x_r)$ be a basis of $I$ and $(x_1, \dotsc, x_r, x_{r+1}, \dotsc, x_s)$ one of $\g$. With respect to the basis $(x_1, \dotsc, x_r)$ of $I$ the endomorphism $\ad_I(x)$ is represented by a matrix $A_x \in \Mat_r(k)$ and $\ad_I(y)$ is represented by a matrix $A_y \in \Mat_r(k)$. As $I \subideal \g$ is in ideal it follows that $\im \ad_\g(x) \subseteq \g$ and $\im \ad_\g(y) \subseteq \g$, which is why with respect to the basis $(x_1, \dotsc, x_s)$ the endomorphism $\ad_\g(x)$ and $\ad_\g(y)$ are represented by a matrices
 \[
  C_x = \begin{pmatrix} A_x & B_x \\ 0 & 0 \end{pmatrix} \in \Mat_s(k)
  \quad\text{and}\quad
  C_y = \begin{pmatrix} A_y & B_y \\ 0 & 0 \end{pmatrix} \in \Mat_s(k)
 \]
 for some matrices $B_x, B_y \in \Mat_{s-r,r}(k)$. Hence
 \begin{align*}
  \kappa_\g(x,y)
  &= \tr(\ad_\g(x) \ad_\g(y))
  = \tr\left( \begin{pmatrix} A_x & B_x \\ 0 & 0 \end{pmatrix} \begin{pmatrix} A_y & B_y \\ 0 & 0 \end{pmatrix} \right) \\
  &= \tr \begin{pmatrix} A_x A_y & A_x B_y \\ 0 & 0 \end{pmatrix}
  = \tr(A_x A_y)
  = \tr(\ad_I(x) \ad_I(y))
  = \kappa_I(x,y).
 \qedhere
 \end{align*}
\end{proof}


\begin{expl}
 Let $\g \coloneqq \sll_n(k) = [\gl_n(k), \gl_n(k)]$. As seen in example~\ref{expl: Killing form of gl_n} the Killing form of $\gl_n(k)$ is given by
 \[
  \kappa_{\gl_n(k)}(x,y) = 2n \tr(xy) - 2(\tr x)(\tr y)
  \quad \text{for all $x,y \in \gl_n(k)$}.
 \]
 Because $\sll_n(k) \subideal \gl_n(k)$ it follows from Lemma~\ref{lem: restriction of the Killing form to an ideal} that the Killing form of $\sll_n(k)$ is given by
 \[
  \kappa_{\sll_n(k)}(x,y) = 2n \tr(xy) \quad \text{for all $x,y \in \sll_n(k)$}.
 \]
 In particular the Killing form of $\sll_n(k)$ is just a multiple of the trace form.
\end{expl}


\begin{lem}
 Let $\g$ be a Lie algebra and $I_1, I_2 \subideal \g$ ideals with $\g = I_1 \oplus I_2$. Then $I_1 \perp I_2$ with respect to the Killing form $\kappa$ of $\g$. In particular it follows that for all $x,y \in \g$ with $x = x_1 + x_2$ and $y = y_1 + y_2$ with respect to $\g = I_1 \oplus I_2$
 \[
  \kappa(x,y) = \kappa_{I_1}(x_1, y_1) + \kappa_{I_2}(x_2, y_2)
 \]
\end{lem}
\begin{proof}
 Because $[I_1, I_2] \subseteq I_1 \cap I_2 = 0$ it follows that for every $z_1 \in I_1$ and $z_2 \in I_2$
 \[
  (\ad(z_1)\ad(z_2))(\g) = \ad(z_1)(\ad(z_2)(\g)) \subseteq \ad(z_1)(I_2) = 0.
 \]
 Therefore $\ad(z_1)\ad(z_2) = 0$ and in particular $\kappa(z_1,z_2) = \tr(\ad(z_1)\ad(z_2)) = 0$. From Lemma \ref{lem: restriction of the Killing form to an ideal} it further follows that
 \begin{align*}
  \kappa(x,y)
  &= \kappa(x_1, y_1) + \kappa(x_1, y_2) + \kappa(x_2, y_1) + \kappa(x_2, y_2) \\
  &= \kappa(x_1, y_1) + \kappa(x_2, y_2)
  = \kappa_{I_1}(x_1, y_1) + \kappa_{I_2}(x_2, y_2).
 \end{align*}
\end{proof}



\subsubsection{The concrete Jordan decomposition}


\begin{defi}
 Let $V$ be an $n$-dimensional $k$-vector space and $x \in \End_k(V)$ (resp.\ $y \in \Mat_n(y)$). Then $x$ (resp.\ $y$) is called \emph{semisimple} if it is diagonalizable.
\end{defi}


\begin{rem}
 An endomorphism $x \in \End_k(V)$ as above is semisimple if and only if every $x$-invariant subspace of $V$ has a direct summand which is also $x$-invariant. (This depends on $k$ being algebraically closed.)
\end{rem}


\begin{thrm}\label{thrm: concrete Jordan decomposition}
 Let $V$ be a finite dimensional $k$-vector space and $x \in \End_k(V)$.
 \begin{enumerate}[leftmargin=*]
  \item
   There exist unique $x_s, x_n \in \End_k(V)$ satisfying the following properties:
   \begin{enumerate}
    \item
     $x = x_s + x_n$.
    \item
     $x_s$ is semisimple and $x_n$ is nilpotent.
    \item
     $x_s$ and $x_n$ commute.
   \end{enumerate}
  \item
   $x_s$ and $x_n$ are Polynomials in $x$ without constant term, i.e.\ there exist polynomials $P,Q \in k[T]$ such that $P(0) = Q(0) = 0$ and $x_s = P(x)$ and $x_n = P(x)$. In particular an endomorphism of $V$ commutes with $x$ if and only if it commutes with $x_s$ and $x_n$.
  \item
   If $A \subseteq B \subseteq V$ are linear subspaces with $x(B) \subseteq A$ then also $x_s(B) \subseteq A$ and $x_n(B) \subseteq A$.
 \end{enumerate}
\end{thrm}
\begin{proof}
 Let $\chi(T)$ be the characteristic polynomial of $x$ with $\chi(T) = \prod_{i=1}^n (T-\lambda_i)^{m_i}$ where $\lambda_i \neq \lambda_j$ for $i \neq j$. By the chinese reminder theorem the map
 \begin{equation}\label{eqn: use of chinese reminder theorem}
  \begin{aligned}
   k[T]/(\chi) &\longrightarrow \prod_{i=1}^n k[T]/((T-\lambda_i)^{m_i}),\\
   F + (\chi) &\longmapsto (F+((T-\lambda_1)^{m_1}), \dotsc, F+(((T-\lambda_n)^{m_n}))
  \end{aligned}
 \end{equation}
 is surjective. Thus there exists some polynomial $P \in k[T]$ with
 \begin{equation}\label{eqn: congruences of P}
  P(T) \equiv \lambda_i \mod (T-\lambda_i)^{m_i} \quad \text{for ever $i = 1, \dotsc, n$}.
 \end{equation}
 
 We can also assume that $P(0) = 0$. If $\lambda_i = 0$ for some $i$ then this follows directly from \eqref{eqn: congruences of P}. Otherwise the polynomials $(T-\lambda_1)^{m_1}$, \dots, $(T-\lambda_n)^{m_n}$, $T$ are pairwise coprime, so by replacing $\chi(T)$ with $\tilde{\chi}(T) \coloneqq \chi(T) T$ in \eqref{eqn: use of chinese reminder theorem} results in a polynomial $\tilde{P}$ which does not only satisfy \eqref{eqn: congruences of P} (with $P$ replaced by $\tilde{P}$) but also $\tilde{P} \bmod T = 0$.
 
 Now let $Q(T) \coloneqq T - P(T)$ as well as $x_s \coloneqq P(x)$ and $x_n \coloneqq Q(x)$. Then $x = x_s + x_n$ and $x_s$ and $x_n$ commute, as both are polynomials in $x$. For every $i = 1, \dotsc, n$ let
 \[
  V_i \coloneqq \ker (x-\lambda_i)^{m_i}
 \]
 be the generalized eigenspace of $x$ with respect to the eigenvalue $\lambda_i$. Is is known from linear algebra that $V = \bigoplus_{i=1}^n V_i$.
 
 It follows from \eqref{eqn: use of chinese reminder theorem} that for every $i = 1, \dotsc, n$ there exists some polynomial \mbox{$P_i \in k[T]$} with
 \[
  P(T) = \lambda_i + P_i(T) (T-\lambda_i)^{m_i},
 \]
 from which follows for every $v \in V_i$ that
 \[
  x_s(v)
  = (\lambda_i \id_V + P_i(x)(x-\lambda_i)^{m_i})(v)
  = \lambda_i v + P_i(x)(\underbrace{(x-\lambda_i)^{m_i}(v)}_{=0})
  = \lambda_i v.
 \]
 Hence $V_i$ is $x_s$-invariant with $x_s|_{V_i} = \lambda_i \id_{V_i}$ for every $i = 1, \dotsc, n$. As $V = \bigoplus_{i=1}^n V_i$ this shows that $x_s$ is semisimple and $V_i$ is precisely the eigenspace of $x_s$ to the eigenvalue $\lambda_i$.
 
 To see that $x_s$ is nilpotent notice that for every $i = 1, \dotsc, n$ and $v \in V_i$
 \[
  x_n(v) = x(v) - x_s(v) = x(v) - \lambda_i (v) = (x - \lambda_i \id_V)(v).
 \]
 Hence $V_i$ is $x_n$-invariant with $x_n|_{V_i} = x - \lambda_i \id_{V_i}$ for every $i = 1, \dotsc, n$. By definition of $V_i$ it follows that $x_n|_{V_i}$ is nilpotent for every $i = 1, \dotsc, n$. Because $V = \bigoplus_{i=1}^n V_i$ it follows that $x_n$ is nilpotent.
 
 This shows the existence of the claimed decomposition. For the uniqueness let $y_s, y_n \in \End_k(V)$ be any two endomorphisms with $x = y_s + y_n$ where $y_s$ is semisimple, $y_n$ is nilpotent and $y_s$ and $y_n$ commute. As $y_s$ and $y_n$ commute it follows that each of them commutes with $x = y_s + y_n$. Because $x_s$ and $x_n$ are polynomials in $x$ it follows from this that $y_s$ and $y_n$ both commute with $x_s$ and $x_n$. Hence $x_s$, $x_n$, $y_s$ and $y_n$ are pairwise commuting. In particular $x_s$ and $y_s$ are simultaneously diagonalizable which is why $x_s - y_s$ is also semisimple. As $x_n$ and $y_n$ commute and are both nilpotent it also follows that $y_n - x_n$ is nilpotent. But from $x_s + x_n = x = y_s + y_n$ it follows that
 \[
  \underbrace{x_s - y_s}_{\text{semisimple}} = \underbrace{y_n - x_n}_{\text{nilpotent}}.
 \]
 Hence $x_s - y_s = y_n - x_n = 0$.
 
 All other statements of the theorem directly follow from the construction of $x_s$ and $x_n$ and the fact that they are polynomials without constant term in $x$.
\end{proof}



\begin{rem}\label{rem: concrete Jordan decomposition for matrices}
 An analogeous statement of Theorem~\ref{thrm: concrete Jordan decomposition} can be shown for $\Mat_n(k)$ instead of $\End_k(V)$. More precisely: Every matrix $x \in \Mat_n(k)$ can be uniquely decomposed into $x = x_s + x_n$ such that $x_s$ is semisimple, $x_n$ is nilpotent and $x_s$ and $x_n$ commute. Both $x_s$ and $x_n$ are polynomials without constant term in $x$, so any matrix in $\Mat_n(k)$ commutes with $x$ if and only if it commutes with $x_s$ and $x_n$. If $A \subseteq B \subseteq k^n$ are linear subspaces such that $B$ is carried into $A$ by left multiplication with $x$, then the same goes for $x_s$ and $x_n$.
\end{rem}


\begin{defi}
 Let $V$ be an $n$-dimensional vector space and $x \in \End_k(V)$ (resp.\ $y \in \Mat_n(k)$). Then the decomposition $x = x_s + x_n$ from Theorem~\ref{thrm: concrete Jordan decomposition} (resp.\ the decomposition $y = y_s + y_n$ from Remark~\ref{rem: concrete Jordan decomposition for matrices}) is called the \emph{concrete Jordandecomposition} of $x$ (resp.\ $y$). The element $x_s$ (resp.\ $y_s$) is called the \emph{semisimple part} of $x$ (resp.\ $y$) and the element $x_n$ (resp.\ $y_n$) is called the \emph{nilpotent part} of $x$ (resp.\ $y$).
\end{defi}


\begin{defi}
 Let $\g$ be a finite dimensional Lie algebra. Then $x \in \g$ is called \emph{$\ad$-semisimple} if $\ad(x)$ is a semisimple endomorphism of $\g$.
\end{defi}


\begin{lem}\label{lem: ss and nilpotent implies ad-ss and ad-nilpotent}
 Let $\g \subseteq \gl(V)$ be a Lie subalgebra for some finite dimensional vector space $V$ (resp.\ $\g \subseteq \gl_n(k)$) and $x \in \g$.
 \begin{enumerate}
  \item
   If $x$ is semisimple then $x$ is also $\ad$-semisimple.
  \item
   If $x$ is nilpotent then $x$ is also $\ad$-nilpotent.
 \end{enumerate}
\end{lem}
\begin{proof}
 We only show the case $\g \subseteq \gl(V)$, the proof for the case $\g \subseteq \gl_n(k)$ being essentially the same.
 \begin{enumerate}[leftmargin=*]
  \item
   Let $e_1$, \dots, $e_n$ be a basis of $\g$ consisting of eigenvectors of $x$, where $e_i$ belongs to the eigenvalue $\lambda_i \in k$. Then for all \mbox{$i,j = 1, \dotsc, n$} let $E_{ij} \in \End_k(\g)$ be defined by
   \[
    E_{ij}(e_k) = \delta_{jk} e_i \quad \text{for every $k = 1, \dotsc, n$}.
   \]
   Then $(E_{ij})_{i,j=1,\dotsc,n}$ is a basis of $\End_k(\g)$. For all $i,j,k = 1, \dotsc, n$ it follows that
   \begin{align*}
    [x,E_{ij}](e_k)
    &= (x E_{ij} - E_{ij} x)(e_k)
    = x(E_{ij}(e_k)) - E_{ij}(x(e_k)) \\
    &= \delta_{jk} x(e_i) - \lambda_k E_{ij}(e_k)
    = \delta_{jk} \lambda_i e_i - \delta_{jk} \lambda_k e_i \\
    &= (\lambda_i - \lambda_j) \delta_{jk} e_i
    = (\lambda_i - \lambda_j) E_{ij} e_k.
   \end{align*}
   It follows that
   \[
    \ad_{\gl(V)}(x)(E_{ij}) = [x,E_{ij}] = (\lambda_i - \lambda_j) E_{ij}
    \quad \text{for all $i,j = 1, \dotsc, n$},
   \]
   so $\ad_{\gl(V)}$ is semisimple and therefore also the restriction $\ad_\g(x) = \ad_{\gl(V)}(x)|_{\g}$.
   
  \item
   In Lemma~\ref{lem: nilpotent implies ad-nilpotent} it was already shown that $\ad_{\gl(V)}(x)$ is nilpotent. From this it follows that the restriction $\ad_\g(x) = \ad_{\gl(V)}(x)|_{\g}$ is also nilpotent.
  \qedhere
 \end{enumerate}
\end{proof}


\begin{cor}\label{cor: concrete Jordan decomposition compatible with adjoint representation}
 Let $\g \subseteq \gl(V)$ be a Lie subalgebra for a finite dimensional vector space $V$ (resp.\ $\g = \gl_n(k)$). If $x \in \g$ has the Jordan decomposition $x = x_s + x_n$ then $\ad(x) = \ad(x_s) + \ad(x_n)$ is the Jordan decomposition of $\ad(x)$.
\end{cor}
\begin{proof}
 As $x_s$ is semisimple the same goes for $\ad(x_s)$ and as $x_n$ is nilpotent the same goes for $\ad(x_n)$, each following from Lemma~\ref{lem: ss and nilpotent implies ad-ss and ad-nilpotent}. As $x_s$ and $x_n$ commute so do $\ad(x_n)$ and $\ad(x_s)$ because $\ad$ is a homomorphism of Lie algebras.
\end{proof}





\subsubsection{Cartanâ€™s Criterion}


\begin{lem}\label{lem: Cartan's criterion technical lemma}
 Let $V$ be a finite dimensional $k$-vector space. Let $A \subseteq B \subseteq \gl(V)$ be linear subspaces and let
 \[
  T \coloneqq \{z \in \gl(V) \mid \ad(z)(B) \subseteq A\}.
 \]
 If $x \in T$ and $\tr(xz) = 0$ for every $z \in T$ then $x$ is nilpotent.
\end{lem}
\begin{proof}
 Let $x = x_s + x_n$ be the concrete Jordan decompositon of $x$. Then the concrete Jordan decomposition of $\ad(x)$ is given by $\ad(x) = \ad(x_s) + \ad(x_n)$ by Corollary~\ref{cor: concrete Jordan decomposition compatible with adjoint representation}. As $\ad(x)(B) \subseteq A$ it follows from the properties of the concrete Jordan decomposition (see Theorem~\ref{thrm: concrete Jordan decomposition}) that also $\ad(x_s)(B) = \ad(x)_s(B) \subseteq A$. Hence $x_s \in T$.
 
 Let $(v_1, \dotsc, v_n)$ be an ordered basis of $V$ with respect to which $x$ is in Jordan normal form. Then with respect to this basis $x_s$ is diagonal and $x_n$ is strictly upper triangular. Let $\lambda_i \in k$ with $x_s(v_i) = \lambda_i v_i$ for every $i = 1, \dotsc, n$ and set
 \[
  E \coloneqq \vspan_\Q(\lambda_1, \dotsc, \lambda_n) \subseteq k.
 \]
 To show that $x$ is nilpotent it sufficies to show that $x_s = 0$, which is equivalent to $\lambda_i = 0$ for every $i = 1, \dotsc, n$. As this is the same as $E = 0$ it is enough to show that $f = 0$ for every $\Q$-linear map $f \colon E \to \Q$. For the rest of the proof we fix such an $f$. Let $z \colon V \to V$ be defined by
 \[
  z(v_i) \coloneqq f(\lambda_i) v_i \quad \text{for every $i = 1, \dotsc, n$}.
 \]
 
 \begin{claim*}
  $z \in T$.
 \end{claim*}
 \begin{proof}
  For all $i,j = 1, \dotsc, n$ let $e_{ij} \in \gl(V)$ with
  \[
   e_{ij}(v_k) = \delta_{jk} v_i \quad \text{for every $k = 1, \dotsc, n$}.
  \]
  Then $(e_{ij})_{i,j = 1, \dotsc, n}$ is a $k$-basis of $\gl(V)$. As already seen in the proof of Corollary~\ref{cor: concrete Jordan decomposition compatible with adjoint representation} this is a basis of eigenvectors of $\ad(x_s)$ where $e_{ij}$ is an eigenvector of $\ad(x_s)$ with respect to the eigenvalue $\lambda_i - \lambda_j$. Because $(v_1, \dotsc, v_n)$ is also a basis of $V$ consisting of eigenvalues of $z$, where $v_i$ belongs to the eigenvalue $f(\lambda_i)$, it follows in the same way, that $e_{ij}$ is an eigenvector of $\ad(z)$ with respect to the eigenvalue
  \[
   \mu_{ij} \coloneqq f(\lambda_i) - f(\lambda_j) = f(\lambda_i - \lambda_j)
  \]
  for all $i,j = 1, \dotsc, n$. In particular it follows that if $e_{ij}$ and $e_{i'j'}$ have the same eigenvalue with respect to $\ad(x_s)$, i.e.\ if $\lambda_i - \lambda_j = \lambda_{i'} - \lambda_{j'}$ then the same goes holds with respect to $\ad(z)$. Hence if $y \in \gl(V)$ is an eigenvector of $\ad(x_s)$ with respect to the eigenvalue $\lambda$ then $y$ is an eigenvector of $\ad(z)$ with respect to the eigenvalue $f(y)$.
  
  As $\ad(x)(B) \subseteq A \subseteq B$ there exists a decomposition $B = A \oplus N$ into linear subspaces with $A$ being $\ad(x)$-invariant and thus decomposing into $\ad(x)$-eigenspaces and $N \subseteq \ker \ad(x)$. Then by the previous observations it follows that $A$ also decomposes into $\ad(z)$-eigenspaces and that $\ad(z)(N) = 0$. Hence $\ad(z)(B) \subseteq A$ and thus $z \in T$.
 \end{proof}
 
 Now \mbox{$\tr(x z) = \tr(x_s z) + \tr(x_n z)$}. As $x_s$ and $z$ are both diagonal with respect to the basis $(v_1, \dotsc, v_n)$ it follows that $\tr(x_s z) = \sum_{i=1}^n f(\lambda_i) \lambda_i$, and as $x_n z$ is also stricly upper triangular it also follows that $\tr(x_n z) = 0$. Together with $z \in T$ this results in
 \[
  0 = \tr(xz) = \sum_{i=1}^n f(\lambda_i) \lambda_i
 \]
 Because $f(\lambda_i) \in \Q$ and $\lambda_i \in E$ for every $i = 1, \dotsc, n$ applying $f$ to this equation results in
 \[
  0 = \sum_{i=1}^n f(\lambda_i)^2.
 \]
 It follows that $f(\lambda_i) = 0$ for every $i = 1, \dotsc, n$ and thus $f = 0$.
\end{proof}


\begin{rem}
 The proof of Lemma~\ref{lem: Cartan's criterion technical lemma} was not actually given in the lecture itself and proving it was an exercise on the third exercise sheet. The proof given above is the one I came up with based on some hints given on the exercise sheet. At some point it will proberly be merged with the proof given in \cite[\S 4.3]{Humphreys}.
 
 In the lecture a proof was given for the special case $k = \Cbb$. But since I have some trouble understanding the details it is not included here (yet).
\end{rem}


\begin{lem}[Cartanâ€™s criterion for $\gl(V)$]
 Let $V$ be a finite dimensional $k$-vector space and $\g \subseteq \gl(V)$ a Lie subalgebra. Then $\g$ is solvable if and only if $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$.
\end{lem}
\begin{proof}
 Suppose that $\g$ is solvable. Then by Lieâ€™s theorem there exists a basis of $V$ with respect to which $\g$ is represented by upper triangular matrices. Then $[\g,\g]$ is represented by strictly upper triangular matrices, which is why $xy$ is also represented by a stricly upper triangular matrix for every $x \in \g$ and $y \in [\g,\g]$. Hence $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$.
 
 Now suppose that $\tr(xy) = 0$ for every $x \in \g$ and $y \in [\g,\g]$. Set $A \coloneqq [\g,\g]$, $B \coloneqq \g$ and
 \[
  T \coloneqq \{x \in \gl(V) \mid \ad(x)(B) \subseteq A\}.
 \]
 Let $x \in [\g,\g] \subseteq T$ and $z \in T$. Then $[z,\g] = [z,B] \subseteq A = [\g,\g]$. Writing $x$ as $x = \sum_{i=1}^n [a_i, b_i]$ with $a_i, b_i \in \g$ for every $i = 1, \dotsc, n$ it follows that
 \begin{align*}
  \tr(xz)
  &= \sum_{i=1}^n \tr([a_i, b_i] z)
  = \sum_{i=1}^n \kappa_{\gl(V)}([a_i, b_i], z) \\
  &= \sum_{i=1}^n \kappa_{\gl(V)}(a_i, [b_i, z])
  = \sum_{i=1}^n \tr(a_i \underbrace{[b_i, z]}_{\in [\g,\g]})
  = 0,
 \end{align*}
 where the last step uses the asumption. It follows from Lemma~\ref{lem: Cartan's criterion technical lemma} that $x$ is nilpotent. Because $[\g,\g]$ consists of nilpotent elements there exists a basis of $V$ with respect to which $[\g,\g]$ is represented by stricly upper triangular matrices. Hence $[\g,\g]$ is nilpotent und $\g$ therefore solvable.
\end{proof}


\begin{thrm}[Cartanâ€™s criterion for solvability]
 Let $\g$ be a finite dimensional Lie algebra. Then $\g$ is solvable if and only if
 \[
  \kappa(x,y) = 0 \quad \text{for every $x \in \g$ and $y \in [\g,\g]$}.
 \]
\end{thrm}
\begin{proof}
 Because $Z(\g)$ is a solvable ideal in $\g$ it follows that $\g$ is solvable if and only if $\g/Z(\g) \cong \ad \g \subseteq \gl(\g)$ is solvable. By Cartanâ€™s criterion for $\gl(\g)$ this is the case if and only if
 \[
  \tr(xy) = 0 \quad \text{for every $x \in \ad \g$ and $y \in [\ad(\g), \ad(\g)]$}.
 \]
 Because $[\ad(\g), \ad(\g)] = \ad([\g,\g])$ and $\tr(\ad(x)\ad(y)) = \kappa(x,y)$ for all $x,y \in \g$ this is equivalent to
 \[
  \kappa(x,y) = 0 \quad \text{for every $x \in \g$ and $y \in [\g,\g]$}.
  \qedhere
 \]
\end{proof}


\begin{cor}\label{cor: rad kappa is a solvable ideal}
 Let $\g$ be a finite dimensional Lie algebra and $\kappa$ the Killing form of $\g$. Then $\rad \kappa$ is a solvable ideal of $\g$. In particular $\rad \kappa \subseteq \rad \g$.
\end{cor}
\begin{proof}
 Lemma~\ref{lem: orthogonal complement of an ideal is again an ideal} already showed that $\rad \kappa$ is an ideal in $\g$. From Lemma~\ref{lem: restriction of the Killing form to an ideal} and the definition of $\rad \kappa$ it follows that 
 \[
  \kappa_{\rad \g}(x,y) = \kappa(x,y) = 0 \quad \text{for all $x,y \in \rad \kappa$}.
 \]
 Hence by Cartanâ€™s criterion $\rad \kappa$ is solvable.
\end{proof}
















